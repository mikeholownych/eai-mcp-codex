"""
The main purpose of this module is to expose LinkCollector.collect_sources().
"""

import collections
import email.message
import functools
import itertools
import json
import logging
import os
import urllib.parse
import urllib.request
from html.parser import HTMLParser
from optparse import Values
from typing import (
    TYPE_CHECKING,
    Callable,
    Dict,
    Iterable,
    List,
    MutableMapping,
    NamedTuple,
    Optional,
    Sequence,
    Tuple,
    Union,
)

from pip._vendor import requests
from pip._vendor.requests import Response
from pip._vendor.requests.exceptions import RetryError, SSLError

from pip._internal.exceptions import NetworkConnectionError
from pip._internal.models.link import Link
from pip._internal.models.search_scope import SearchScope
from pip._internal.network.session import PipSession
from pip._internal.network.utils import raise_for_status
from pip._internal.utils.filetypes import is_archive_file
from pip._internal.utils.misc import redact_auth_from_url
from pip._internal.vcs import vcs

from .sources import CandidatesFromPage, LinkSource, build_source

if TYPE_CHECKING:
    from typing import Protocol
else:
    Protocol = object

logger = logging.getLogger(__name__)

ResponseHeaders = MutableMapping[str, str]


def _match_vcs_scheme(url: str) -> Optional[str]:
    """Look for VCS schemes in the URL.

    Returns the matched VCS scheme, or None if there's no match.
    """
    for scheme in vcs.schemes:
        if url.lower().startswith(scheme) and url[len(scheme)] in "+:":
            return scheme
    return None


class _NotAPIContent(Exception):
    def __init__(self, content_type: str, request_desc: str) -> None:
        """
        Initialize the exception indicating a response did not have an expected API Content-Type.
        
        Parameters:
            content_type (str): The Content-Type header value received.
            request_desc (str): A brief description of the request or resource checked.
        
        Attributes:
            content_type (str): Same as the `content_type` argument.
            request_desc (str): Same as the `request_desc` argument.
        """
        super().__init__(content_type, request_desc)
        self.content_type = content_type
        self.request_desc = request_desc


def _ensure_api_header(response: Response) -> None:
    """
    Ensure the HTTP response has a Simple API Content-Type.
    
    Checks the response's Content-Type header and allows the following types:
    `text/html`, `application/vnd.pypi.simple.v1+html`, and
    `application/vnd.pypi.simple.v1+json`. If the header does not start with one
    of these values, raises `_NotAPIContent` with the response Content-Type and
    the request method as the error details.
    """
    content_type = response.headers.get("Content-Type", "Unknown")

    content_type_l = content_type.lower()
    if content_type_l.startswith(
        (
            "text/html",
            "application/vnd.pypi.simple.v1+html",
            "application/vnd.pypi.simple.v1+json",
        )
    ):
        return

    raise _NotAPIContent(content_type, response.request.method)


class _NotHTTP(Exception):
    pass


def _ensure_api_response(url: str, session: PipSession) -> None:
    """
    Perform a HEAD request to verify the URL returns a Simple API-compatible response.
    
    Only HTTP/HTTPS schemes are supported; a non-http(s) URL raises _NotHTTP.
    If the response status is an HTTP error, raise_for_status() will raise an HTTPError.
    If the response Content-Type is not an allowed Simple API type, _ensure_api_header will raise _NotAPIContent.
    """
    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)
    if scheme not in {"http", "https"}:
        raise _NotHTTP()

    resp = session.head(url, allow_redirects=True)
    raise_for_status(resp)

    _ensure_api_header(resp)


def _get_simple_response(url: str, session: PipSession) -> Response:
    """
    Fetch a PyPI Simple API page with GET and return the HTTP response.
    
    Performs an optional pre-check (HEAD) for URLs that look like archive files to avoid downloading large non-API blobs, then issues a GET request with Accept headers preferring the Simple API JSON/HTML media types and Cache-Control: max-age=0 to allow conditional requests while avoiding stale cached results. After the GET, validates the response Content-Type and raises if it is not a recognized Simple API or HTML type.
    
    Raises:
        _NotHTTP: If the URL scheme prevents a HEAD-based check.
        _NotAPIContent: If the response Content-Type is not HTML or a supported Simple API media type.
        requests.HTTPError: For 4xx/5xx responses (via raise_for_status).
    """
    if is_archive_file(Link(url).filename):
        _ensure_api_response(url, session=session)

    logger.debug("Getting page %s", redact_auth_from_url(url))

    resp = session.get(
        url,
        headers={
            "Accept": ", ".join(
                [
                    "application/vnd.pypi.simple.v1+json",
                    "application/vnd.pypi.simple.v1+html; q=0.1",
                    "text/html; q=0.01",
                ]
            ),
            # We don't want to blindly returned cached data for
            # /simple/, because authors generally expecting that
            # twine upload && pip install will function, but if
            # they've done a pip install in the last ~10 minutes
            # it won't. Thus by setting this to zero we will not
            # blindly use any cached data, however the benefit of
            # using max-age=0 instead of no-cache, is that we will
            # still support conditional requests, so we will still
            # minimize traffic sent in cases where the page hasn't
            # changed at all, we will just always incur the round
            # trip for the conditional GET now instead of only
            # once per 10 minutes.
            # For more information, please see pypa/pip#5670.
            "Cache-Control": "max-age=0",
        },
    )
    raise_for_status(resp)

    # The check for archives above only works if the url ends with
    # something that looks like an archive. However that is not a
    # requirement of an url. Unless we issue a HEAD request on every
    # url we cannot know ahead of time for sure if something is a
    # Simple API response or not. However we can check after we've
    # downloaded it.
    _ensure_api_header(resp)

    logger.debug(
        "Fetched page %s as %s",
        redact_auth_from_url(url),
        resp.headers.get("Content-Type", "Unknown"),
    )

    return resp


def _get_encoding_from_headers(headers: ResponseHeaders) -> Optional[str]:
    """
    Return the charset encoding declared in the response headers' Content-Type, if any.
    
    Checks the 'Content-Type' header for a `charset` parameter and returns it as a string.
    Returns None when there is no Content-Type header or no charset parameter.
    
    Parameters:
        headers (ResponseHeaders): Mapping-like object of HTTP response headers.
    
    Returns:
        Optional[str]: The charset value (e.g. 'utf-8') if present, otherwise None.
    """
    if headers and "Content-Type" in headers:
        m = email.message.Message()
        m["content-type"] = headers["Content-Type"]
        charset = m.get_param("charset")
        if charset:
            return str(charset)
    return None


class CacheablePageContent:
    def __init__(self, page: "IndexContent") -> None:
        """
        Initialize a cacheable wrapper for an IndexContent used as a cache key.
        
        The wrapper asserts that the wrapped IndexContent has cache_link_parsing enabled
        and stores a reference to it. Instances are compared/hashed by the page URL to
        allow deduplication in caching.
        """
        assert page.cache_link_parsing
        self.page = page

    def __eq__(self, other: object) -> bool:
        """
        Return True if `other` is the same type and wraps a page with the same URL.
        
        Compares equality by ensuring `other` is an instance of the same class and that
        both objects refer to pages with identical `url` values.
        
        Returns:
            bool: True when types match and `page.url` is equal; otherwise False.
        """
        return isinstance(other, type(self)) and self.page.url == other.page.url

    def __hash__(self) -> int:
        """
        Return a hash based on the wrapped page's URL.
        
        This uses the IndexContent.url as the identity for caching/deduplication so
        CacheablePageContent instances with the same page URL hash to the same value.
        
        Returns:
            int: The hash of the page URL.
        """
        return hash(self.page.url)


class ParseLinks(Protocol):
    def __call__(self, page: "IndexContent") -> Iterable[Link]:
        """
        Parse an IndexContent (a fetched simple API page) and yield Link objects for each discovered file or anchor.
        
        Parameters:
            page (IndexContent): The fetched page bytes, content type, encoding, and URL to parse.
        
        Returns:
            Iterable[Link]: An iterable of Link objects found in the page.
        """
        ...


def with_cached_index_content(fn: ParseLinks) -> ParseLinks:
    """
    Decorator that adds per-page caching to a link-parsing function.
    
    Wraps a ParseLinks function so its results are cached (using an unbounded LRU cache)
    when the provided IndexContent has page.cache_link_parsing == True. Caching is keyed
    by CacheablePageContent (which deduplicates by page URL), and the wrapped function
    always returns a list of Link objects.
    
    If page.cache_link_parsing is False, the underlying parser is invoked directly and
    its results are not cached.
    """

    @functools.lru_cache(maxsize=None)
    def wrapper(cacheable_page: CacheablePageContent) -> List[Link]:
        """
        Call the underlying parse function with the wrapped IndexContent and return its results as a list.
        
        Parameters:
            cacheable_page (CacheablePageContent): Wrapper holding the IndexContent to parse.
        
        Returns:
            List[Link]: A list of Link objects produced by the wrapped parse function.
        """
        return list(fn(cacheable_page.page))

    @functools.wraps(fn)
    def wrapper_wrapper(page: "IndexContent") -> List[Link]:
        """
        Return a list of parsed Link objects for the given IndexContent, using the cache when allowed.
        
        If page.cache_link_parsing is True, retrieve parsed links from the decorator's LRU cache (via
        CacheablePageContent). Otherwise, parse the page directly by calling the wrapped parse function.
        
        Parameters:
            page (IndexContent): The fetched page to parse. Its `cache_link_parsing` attribute controls
                whether cached results are used.
        
        Returns:
            List[Link]: The parsed links extracted from the page.
        """
        if page.cache_link_parsing:
            return wrapper(CacheablePageContent(page))
        return list(fn(page))

    return wrapper_wrapper


@with_cached_index_content
def parse_links(page: "IndexContent") -> Iterable[Link]:
    """
    Parse a Simple API index page and yield parsed Link objects.
    
    This accepts an IndexContent for a Simple API page and produces Link objects
    for each file or anchor found.
    
    Behavior:
    - If the Content-Type indicates `application/vnd.pypi.simple.v1+json`, the
      content is parsed as JSON and each entry in the top-level `files` array is
      converted to a Link via `Link.from_json`.
    - Otherwise the content is treated as HTML. It is decoded using
      `page.encoding` if set, or UTF-8 by default, parsed for `<a>` elements with
      `HTMLLinkParser`, and each anchor is converted to a Link via
      `Link.from_element` using the page URL and any discovered base URL.
    
    Invalid or unrecognizable entries are skipped (no exception raised for those
    entries).
    
    Parameters:
        page (IndexContent): The fetched page and metadata to parse.
    
    Yields:
        Link: Parsed Link objects for each valid file or anchor in the page.
    """

    content_type_l = page.content_type.lower()
    if content_type_l.startswith("application/vnd.pypi.simple.v1+json"):
        data = json.loads(page.content)
        for file in data.get("files", []):
            link = Link.from_json(file, page.url)
            if link is None:
                continue
            yield link
        return

    parser = HTMLLinkParser(page.url)
    encoding = page.encoding or "utf-8"
    parser.feed(page.content.decode(encoding))

    url = page.url
    base_url = parser.base_url or url
    for anchor in parser.anchors:
        link = Link.from_element(anchor, page_url=url, base_url=base_url)
        if link is None:
            continue
        yield link


class IndexContent:
    """Represents one response (or page), along with its URL"""

    def __init__(
        self,
        content: bytes,
        content_type: str,
        encoding: Optional[str],
        url: str,
        cache_link_parsing: bool = True,
    ) -> None:
        """
        Represent a fetched index page and its metadata.
        
        Parameters:
            content (bytes): Raw response body of the fetched page.
            content_type (str): Value of the response Content-Type header.
            encoding (Optional[str]): Character encoding to decode `content`, if known.
            url (str): The page URL (used for resolving relative links and as a cache key).
            cache_link_parsing (bool): Whether parsed links from this page should be cached;
                set False for pages that should not be memoized (e.g., PyPI index pages).
        """
        self.content = content
        self.content_type = content_type
        self.encoding = encoding
        self.url = url
        self.cache_link_parsing = cache_link_parsing

    def __str__(self) -> str:
        """
        Return a redacted string representation of the IndexContent.
        
        The returned string is the page URL with any embedded authentication credentials removed
        (same format as redact_auth_from_url). This does not modify the object.
        Returns:
            A URL string with credentials redacted.
        """
        return redact_auth_from_url(self.url)


class HTMLLinkParser(HTMLParser):
    """
    HTMLParser that keeps the first base HREF and a list of all anchor
    elements' attributes.
    """

    def __init__(self, url: str) -> None:
        """
        Initialize the HTMLLinkParser.
        
        Parameters:
            url (str): The page URL used as the context for resolving relative links.
            
        The parser starts with no base URL (will be set if a <base> tag is encountered)
        and an empty list of anchor attribute dicts that will be populated as <a>
        elements are parsed.
        """
        super().__init__(convert_charrefs=True)

        self.url: str = url
        self.base_url: Optional[str] = None
        self.anchors: List[Dict[str, Optional[str]]] = []

    def handle_starttag(self, tag: str, attrs: List[Tuple[str, Optional[str]]]) -> None:
        """
        Handle start tags parsed from HTML.
        
        Sets the parser's base_url from the first encountered <base href="..."> tag (no-op if base_url is already set). For each <a> tag, records the tag's attributes as a dict appended to the parser's anchors list.
        
        Parameters:
            tag: The name of the start tag (e.g., "a", "base").
            attrs: A list of (name, value) attribute pairs for the tag; values may be None.
        """
        if tag == "base" and self.base_url is None:
            href = self.get_href(attrs)
            if href is not None:
                self.base_url = href
        elif tag == "a":
            self.anchors.append(dict(attrs))

    def get_href(self, attrs: List[Tuple[str, Optional[str]]]) -> Optional[str]:
        """
        Return the value of the first "href" attribute found in a list of HTML tag attributes.
        
        Parameters:
            attrs (List[Tuple[str, Optional[str]]]): Sequence of (name, value) attribute pairs from a start tag.
        
        Returns:
            Optional[str]: The value associated with the first "href" attribute, or None if no "href" is present.
        """
        for name, value in attrs:
            if name == "href":
                return value
        return None


def _handle_get_simple_fail(
    link: Link,
    reason: Union[str, Exception],
    meth: Optional[Callable[..., None]] = None,
) -> None:
    """
    Log a failed attempt to fetch a simple index URL and note that it will be skipped.
    
    Parameters:
        link: The Link whose URL could not be fetched.
        reason: Short explanation or exception instance describing the failure.
        meth: Optional logging-like callable to use (defaults to logger.debug). It should accept
            a printf-style format string followed by positional arguments.
    """
    if meth is None:
        meth = logger.debug
    meth("Could not fetch URL %s: %s - skipping", link, reason)


def _make_index_content(
    response: Response, cache_link_parsing: bool = True
) -> IndexContent:
    """
    Create an IndexContent object from an HTTP response.
    
    Extracts the response body, Content-Type header, encoding (from the response headers), and URL, and wraps them in an IndexContent. The cache_link_parsing flag is passed through to control whether parsed links from this page may be cached.
    
    Parameters:
        response (Response): The HTTP response to convert.
        cache_link_parsing (bool): Whether link parsing results for this page are permitted to be cached.
    
    Returns:
        IndexContent: A populated IndexContent instance built from the response.
    """
    encoding = _get_encoding_from_headers(response.headers)
    return IndexContent(
        response.content,
        response.headers["Content-Type"],
        encoding=encoding,
        url=response.url,
        cache_link_parsing=cache_link_parsing,
    )


def _get_index_content(link: Link, *, session: PipSession) -> Optional["IndexContent"]:
    """
    Fetch and return an IndexContent for a given Link, or None if it cannot be retrieved.
    
    Attempts to GET the simple API page referenced by `link` (stripping any fragment). Behaviors:
    - Returns an IndexContent on a successful fetch and validation of the response.
    - Returns None and logs a warning on unsupported VCS schemes, non-HTTP lookups, unsupported Content-Types, network/SSL errors, timeouts, or other failures.
    - For file:// URLs that point to a directory, resolves to an index.html within that directory before fetching.
    
    Parameters:
        link: The Link describing the target location to fetch (may include fragment; fragment is ignored).
    
    Returns:
        An IndexContent instance when the page was successfully retrieved and validated; otherwise None.
    """
    url = link.url.split("#", 1)[0]

    # Check for VCS schemes that do not support lookup as web pages.
    vcs_scheme = _match_vcs_scheme(url)
    if vcs_scheme:
        logger.warning(
            "Cannot look at %s URL %s because it does not support lookup as web pages.",
            vcs_scheme,
            link,
        )
        return None

    # Tack index.html onto file:// URLs that point to directories
    scheme, _, path, _, _, _ = urllib.parse.urlparse(url)
    if scheme == "file" and os.path.isdir(urllib.request.url2pathname(path)):
        # add trailing slash if not present so urljoin doesn't trim
        # final segment
        if not url.endswith("/"):
            url += "/"
        # TODO: In the future, it would be nice if pip supported PEP 691
        #       style responses in the file:// URLs, however there's no
        #       standard file extension for application/vnd.pypi.simple.v1+json
        #       so we'll need to come up with something on our own.
        url = urllib.parse.urljoin(url, "index.html")
        logger.debug(" file: URL is directory, getting %s", url)

    try:
        resp = _get_simple_response(url, session=session)
    except _NotHTTP:
        logger.warning(
            "Skipping page %s because it looks like an archive, and cannot "
            "be checked by a HTTP HEAD request.",
            link,
        )
    except _NotAPIContent as exc:
        logger.warning(
            "Skipping page %s because the %s request got Content-Type: %s. "
            "The only supported Content-Types are application/vnd.pypi.simple.v1+json, "
            "application/vnd.pypi.simple.v1+html, and text/html",
            link,
            exc.request_desc,
            exc.content_type,
        )
    except NetworkConnectionError as exc:
        _handle_get_simple_fail(link, exc)
    except RetryError as exc:
        _handle_get_simple_fail(link, exc)
    except SSLError as exc:
        reason = "There was a problem confirming the ssl certificate: "
        reason += str(exc)
        _handle_get_simple_fail(link, reason, meth=logger.info)
    except requests.ConnectionError as exc:
        _handle_get_simple_fail(link, f"connection error: {exc}")
    except requests.Timeout:
        _handle_get_simple_fail(link, "timed out")
    else:
        return _make_index_content(resp, cache_link_parsing=link.cache_link_parsing)
    return None


class CollectedSources(NamedTuple):
    find_links: Sequence[Optional[LinkSource]]
    index_urls: Sequence[Optional[LinkSource]]


class LinkCollector:

    """
    Responsible for collecting Link objects from all configured locations,
    making network requests as needed.

    The class's main method is its collect_sources() method.
    """

    def __init__(
        self,
        session: PipSession,
        search_scope: SearchScope,
    ) -> None:
        """
        Initialize the LinkCollector.
        
        session is the PipSession used for network requests and origin/security checks.
        search_scope provides the configured index and find-links locations (and related
        flags) that the collector will consult when building source lists.
        """
        self.search_scope = search_scope
        self.session = session

    @classmethod
    def create(
        cls,
        session: PipSession,
        options: Values,
        suppress_no_index: bool = False,
    ) -> "LinkCollector":
        """
        Create a LinkCollector configured from CLI-style options.
        
        Builds a SearchScope from options.index_url, options.extra_index_urls, and
        options.find_links, honoring the --no-index flag unless suppressed, and
        returns a LinkCollector using the provided session and constructed scope.
        
        Parameters:
            options: Parsed options/values providing index_url, extra_index_urls,
                find_links, and no_index.
            suppress_no_index: If True, ignore options.no_index when constructing the
                SearchScope (used by callers that want to override the CLI flag).
        
        Returns:
            LinkCollector: A collector configured with the given session and search scope.
        """
        index_urls = [options.index_url] + options.extra_index_urls
        if options.no_index and not suppress_no_index:
            logger.debug(
                "Ignoring indexes: %s",
                ",".join(redact_auth_from_url(url) for url in index_urls),
            )
            index_urls = []

        # Make sure find_links is a list before passing to create().
        find_links = options.find_links or []

        search_scope = SearchScope.create(
            find_links=find_links,
            index_urls=index_urls,
            no_index=options.no_index,
        )
        link_collector = LinkCollector(
            session=session,
            search_scope=search_scope,
        )
        return link_collector

    @property
    def find_links(self) -> List[str]:
        """
        Return the configured "find-links" locations.
        
        Returns:
            List[str]: A list of user-supplied URLs or filesystem paths from the search scope's
            find_links configuration (used to discover packages outside package indexes).
        """
        return self.search_scope.find_links

    def fetch_response(self, location: Link) -> Optional[IndexContent]:
        """
        Fetch an HTML page containing package links.
        """
        return _get_index_content(location, session=self.session)

    def collect_sources(
        self,
        project_name: str,
        candidates_from_page: CandidatesFromPage,
    ) -> CollectedSources:
        # The OrderedDict calls deduplicate sources by URL.
        """
        Collect link sources for a project from configured index URLs and find-links.
        
        Builds two ordered, deduplicated sequences of LinkSource candidates:
        - index_urls: locations from the configured index URLs (no directory expansion; link-parsing cache disabled).
        - find_links: locations from configured find-links (directory expansion enabled; link-parsing cache enabled).
        
        Each source is created via build_source with the session's security validator (session.is_secure_origin) and the provided candidates_from_page callback.
        
        Parameters:
            project_name: Project name used to resolve index URL locations.
            candidates_from_page: Callable used to produce candidate links from a fetched index page.
        
        Returns:
            CollectedSources: Named tuple with two sequences:
                - find_links: list of LinkSource or None for each configured find-link location (order preserved, deduplicated by URL).
                - index_urls: list of LinkSource or None for each configured index URL location (order preserved, deduplicated by URL).
        """
        index_url_sources = collections.OrderedDict(
            build_source(
                loc,
                candidates_from_page=candidates_from_page,
                page_validator=self.session.is_secure_origin,
                expand_dir=False,
                cache_link_parsing=False,
                project_name=project_name,
            )
            for loc in self.search_scope.get_index_urls_locations(project_name)
        ).values()
        find_links_sources = collections.OrderedDict(
            build_source(
                loc,
                candidates_from_page=candidates_from_page,
                page_validator=self.session.is_secure_origin,
                expand_dir=True,
                cache_link_parsing=True,
                project_name=project_name,
            )
            for loc in self.find_links
        ).values()

        if logger.isEnabledFor(logging.DEBUG):
            lines = [
                f"* {s.link}"
                for s in itertools.chain(find_links_sources, index_url_sources)
                if s is not None and s.link is not None
            ]
            lines = [
                f"{len(lines)} location(s) to search "
                f"for versions of {project_name}:"
            ] + lines
            logger.debug("\n".join(lines))

        return CollectedSources(
            find_links=list(find_links_sources),
            index_urls=list(index_url_sources),
        )
