# docker-compose.prod.yml - Production Overrides
services:
  # Production-specific overrides
  model-router:
    build:
      context: .
      dockerfile: docker/model-router.Dockerfile
    environment:
      - LOG_LEVEL=INFO
      - DEBUG_MODE=false
      - WORKERS=4
      - ANTHROPIC_API_KEY=YOUR_ANTHROPIC_API_KEY
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    restart: always
    depends_on:
      mcp-fluentd:
        condition: service_healthy
    logging:
      driver: "fluentd"
      options:
        fluentd-address: "127.0.0.1:24224"
        tag: "{{.Name}}"

  plan-management:
    build:
      context: .
      dockerfile: docker/plan-management.Dockerfile
    environment:
      - LOG_LEVEL=INFO
      - AUTO_BACKUP=true
      - BACKUP_INTERVAL=1800
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 768M
          cpus: '0.8'
        reservations:
          memory: 384M
          cpus: '0.4'
    restart: always

  git-worktree-manager:
    build:
      context: .
      dockerfile: docker/git-worktree.Dockerfile
    environment:
      - LOG_LEVEL=INFO
      - AUTO_CLEANUP=true
      - CLEANUP_INTERVAL=3600
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.6'
        reservations:
          memory: 256M
          cpus: '0.3'
    restart: always

  workflow-orchestrator:
    build:
      context: .
      dockerfile: docker/workflow-orchestrator.Dockerfile
    environment:
      - LOG_LEVEL=INFO
      - MAX_CONCURRENT_WORKFLOWS=5
      - STEP_TIMEOUT=3600
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    restart: always
    networks:
      - mcp-network
    depends_on:
      mcp-fluentd:
        condition: service_healthy

  verification-feedback:
    build:
      context: .
      dockerfile: docker/verification-feedback.Dockerfile
    environment:
      - LOG_LEVEL=INFO
      - STRICT_VERIFICATION=true
      - PARALLEL_VERIFICATION=false
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 768M
          cpus: '0.8'
        reservations:
          memory: 384M
          cpus: '0.4'
    restart: always
    networks:
      - mcp-network
    depends_on:
      mcp-fluentd:
        condition: service_healthy

  # Production fluentd
  mcp-fluentd:
    image: fluent/fluentd:v1.16
    container_name: mcp-fluentd
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    volumes:
      - ./fluentd/conf:/fluentd/etc
    command: ["fluentd", "-c", "/fluentd/etc/fluent.conf", "-v"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "fluentd --dry-run -c /fluentd/etc/fluent.conf || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      mcp-network:
        aliases:
          - fluentd

  # Production database with performance tuning
  postgres:
    image: postgres:15-alpine
    container_name: mcp-postgres
    restart: unless-stopped

    environment:
      POSTGRES_DB: mcp_database
      POSTGRES_USER: mcp_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-mcp_secure_password}
      POSTGRES_INITDB_ARGS: --auth-host=scram-sha-256
      PGDATA: /var/lib/postgresql/data/pgdata

    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - ./database/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro

    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c work_mem=4MB
      -c maintenance_work_mem=64MB
      -c random_page_cost=1.1
      -c temp_file_limit=2GB
      -c log_min_duration_statement=1000
      -c log_connections=on
      -c log_disconnections=on
      -c log_lock_waits=on

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mcp_user -d mcp_database"]
      interval: 10s
      timeout: 5s
      retries: 5

    ports:
      - "5432:5432"

    logging:
      driver: "fluentd"
      options:
        fluentd-address: "127.0.0.1:24224"
        tag: "mcp-postgres"

    networks:
      - mcp-network

    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.5'
        reservations:
          memory: 1G
          cpus: '0.75'

    depends_on:
      mcp-fluentd:
        condition: service_healthy

  # Production Redis with persistence
  redis:
    image: redis:7-alpine
    container_name: mcp-redis
    restart: always

    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 300
      --timeout 0

    ports:
      - "6379:6379"

    volumes:
      - redis_data:/data

    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

    logging:
      driver: "fluentd"
      options:
        fluentd-address: "127.0.0.1:24224"
        tag: "mcp-redis"

    networks:
      - mcp-network

    deploy:
      resources:
        limits:
          memory: 768M
          cpus: '0.5'
        reservations:
          memory: 384M
          cpus: '0.25'

  # Production Nginx with SSL
  nginx:
    image: nginx:alpine
    volumes:
      - ./nginx/prod.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      frontend:
        condition: service_healthy
      model-router:
        condition: service_healthy
      plan-management:
        condition: service_healthy
      git-worktree-manager:
        condition: service_healthy
      workflow-orchestrator:
        condition: service_healthy
      verification-feedback:
        condition: service_healthy
      staff-service:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
    restart: always
    networks:
      - mcp-network

  # Customer Frontend React/Next.js Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: mcp-frontend
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://localhost/api
      - PORT=3000
    ports:
      - "3002:3000"
    volumes:
      - ./frontend/.next:/app/.next
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Staff Frontend React/Next.js Application
  staff-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: mcp-staff-frontend
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://localhost/api/staff
      - PORT=3001
    ports:
      - "3001:3001"
    volumes:
      - ./frontend/.next:/app/.next
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Remove development tools in production
  dev-tools:
    profiles:
      - dev

networks:
  mcp-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.50.0.0/16

volumes:
  redis_data:
    driver: local
  rabbitmq_data:
    driver: local
  postgres_data:
    driver: local
  consul_data:
    driver: local
  plan_storage:
    driver: local
  git_repositories:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  elasticsearch_data:
    driver: local