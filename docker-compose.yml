# Docker Compose v2 Configuration for MCP Server Microservices
services:
  # Fluentd for Logging Requirements
  mcp-fluentd:
    image: fluent/fluentd:v1.16
    container_name: mcp-fluentd
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    volumes:
      - ./fluentd/conf:/fluentd/etc
    command: ["fluentd", "-c", "/fluentd/etc/fluent.conf", "-v"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "fluentd --dry-run -c /fluentd/etc/fluent.conf || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      mcp-network:
        aliases:
          - fluentd

  # Message Broker for Inter-Service Communication
  redis:
    image: redis:7-alpine
    container_name: mcp-redis
    restart: unless-stopped
    ports:
     - "6379:6379"
    volumes:
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - mcp-network
    depends_on:
      mcp-fluentd:
        condition: service_healthy
    logging:
      driver: "json-file"

  # Database for Shared Data
  postgres:
    image: postgres:15-alpine
    container_name: mcp-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: mcp_database # Default database, specific ones created by init.sql
      POSTGRES_USER: mcp_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-mcp_secure_password}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mcp_user -d mcp_database"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - mcp-network
    depends_on:
      mcp-fluentd:
        condition: service_healthy
    logging:
      driver: "json-file"

  # Service Discovery and Configuration
  consul:
    image: consul:1.15
    container_name: mcp-consul
    restart: unless-stopped
    ports:
      - "8500:8500"
    command: consul agent -dev -client=0.0.0.0 -ui
    volumes:
      - consul_data:/consul/data
    healthcheck:
      test: ["CMD", "consul", "info"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - mcp-network

  # =====================================
  # CORE MCP MICROSERVICES
  # =====================================#

  # Model Router Service
  model-router:
    build:
      context: .
      dockerfile: docker/model-router.Dockerfile
    # container_name: mcp-model-router
    restart: unless-stopped
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ABACUSAI_API_KEY=${ABACUSAI_API_KEY}
      - USE_ABACUS_AI=true
      - PREFER_LOCAL_MODELS=false
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=model-router
      - SERVICE_PORT=8001
      # Model Router does not use a database directly
    # ports:
    #   - "8001:8001"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
    volumes:
      - ./logs/model-router:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Plan Management Service
  plan-management:
    build:
      context: .
      dockerfile: docker/plan-management.Dockerfile
    # container_name: mcp-plan-management
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=plan-management
      - SERVICE_PORT=8002
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/plan_management_db
    # ports:
    #   - "8002:8002"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
    volumes:
      - ./logs/plan-management:/app/logs
      - ./config:/app/config:ro
      - plan_storage:/app/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 384M
          cpus: '0.4'
        reservations:
          memory: 192M
          cpus: '0.2'

  # Git Worktree Manager Service
  git-worktree-manager:
    build:
      context: .
      dockerfile: docker/git-worktree.Dockerfile
    container_name: mcp-git-worktree
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=git-worktree-manager
      - SERVICE_PORT=8003
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/git_worktree_db
    # ports:
    #   - "8003:8003"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
    volumes:
      - ./logs/git-worktree:/app/logs
      - ./config:/app/config:ro
      - git_repositories:/app/repositories
      - /var/run/docker.sock:/var/run/docker.sock:ro  # For Git operations
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'

  # Workflow Orchestrator Service
  workflow-orchestrator:
    build:
      context: .
      dockerfile: docker/workflow-orchestrator.Dockerfile
    # container_name: mcp-workflow-orchestrator
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=workflow-orchestrator
      - SERVICE_PORT=8004
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/workflow_orchestrator_db
      - MODEL_ROUTER_URL=http://model-router:8001
      - PLAN_MANAGEMENT_URL=http://plan-management:8002
      - GIT_WORKTREE_URL=http://git-worktree-manager:8003
      - VERIFICATION_URL=http://verification-feedback:8005
    # ports:
    #   - "8004:8004"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
      model-router:
        condition: service_healthy
    volumes:
      - ./logs/workflow-orchestrator:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.6'
        reservations:
          memory: 256M
          cpus: '0.3'

  # Verification & Feedback Service
  verification-feedback:
    build:
      context: .
      dockerfile: docker/verification-feedback.Dockerfile
    # container_name: mcp-verification-feedback
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=verification-feedback
      - SERVICE_PORT=8005
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/verification_feedback_db
      - MODEL_ROUTER_URL=http://model-router:8001
    # ports:
    #   - "8005:8005"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
      model-router:
        condition: service_healthy
    volumes:
      - ./logs/verification-feedback:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 384M
          cpus: '0.4'
        reservations:
          memory: 192M
          cpus: '0.2'

  # Staff Management Service
  staff-service:
    build:
      context: .
      dockerfile: docker/staff-service.Dockerfile
    # container_name: mcp-staff-service
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=staff-service
      - SERVICE_PORT=8006
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/staff_db
      - STAFF_REQUIRE_AUTH=true
      - STAFF_ALLOWED_ROLES=admin,manager,support
    # ports:
    #   - "8006:8006"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
    volumes:
      - ./logs/staff-service:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8006/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'

  # =====================================
  # AGENT FRAMEWORK SERVICES  
  # =====================================

  # A2A Communication Hub
  a2a-communication:
    build:
      context: .
      dockerfile: docker/a2a-communication.Dockerfile
    container_name: mcp-a2a-communication
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=a2a-communication
      - SERVICE_PORT=8010
      # A2A Communication does not use a database directly
    # ports:
    #   - "8010:8010"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
    volumes:
      - ./logs/a2a-communication:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'

  # Agent Pool Manager
  agent-pool:
    build:
      context: .
      dockerfile: docker/agent-pool.Dockerfile
    container_name: mcp-agent-pool
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=agent-pool
      - SERVICE_PORT=8011
      # Agent Pool does not use a database directly
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
    # ports:
    #   - "8011:8011"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
      a2a-communication:
        condition: service_healthy
    volumes:
      - ./logs/agent-pool:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8011/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 384M
          cpus: '0.4'
        reservations:
          memory: 192M
          cpus: '0.2'

  # Collaboration Orchestrator
  collaboration-orchestrator:
    build:
      context: .
      dockerfile: docker/collaboration-orchestrator.Dockerfile
    container_name: mcp-collaboration-orchestrator
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=collaboration-orchestrator
      - SERVICE_PORT=8012
      # Collaboration Orchestrator does not use a database directly
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_POOL_URL=http://agent-pool:8011
    # ports:
    #   - "8012:8012"
    depends_on:
      - redis
      - consul
      - a2a-communication
      - agent-pool
    volumes:
      - ./logs/collaboration-orchestrator:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8012/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Specialized Agent Services
  planner-agent:
    build:
      context: .
      dockerfile: docker/planner-agent.Dockerfile
    container_name: mcp-planner-agent
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=planner-agent
      - SERVICE_PORT=8013
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_ID=planner-001
      - AGENT_NAME=Primary-Planner
    # ports:
    #   - "8013:8013"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
      a2a-communication:
        condition: service_healthy
    volumes:
      - ./logs/planner-agent:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8013/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'

  security-agent:
    build:
      context: .
      dockerfile: docker/security-agent.Dockerfile
    container_name: mcp-security-agent
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=security-agent
      - SERVICE_PORT=8014
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_ID=security-001
      - AGENT_NAME=Primary-Security
    # ports:
    #   - "8014:8014"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
      a2a-communication:
        condition: service_healthy
    volumes:
      - ./logs/security-agent:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8014/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'

  developer-agent:
    build:
      context: .
      dockerfile: docker/developer-agent.Dockerfile
    container_name: mcp-developer-agent
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=developer-agent
      - SERVICE_PORT=8015
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_ID=developer-001
      - AGENT_NAME=Primary-Developer
      - SPECIALIZATIONS=python,javascript,fastapi
    # ports:
    #   - "8015:8015"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
      a2a-communication:
        condition: service_healthy
    volumes:
      - ./logs/developer-agent:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8015/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 384M
          cpus: '0.4'
        reservations:
          memory: 192M
          cpus: '0.2'

  # Agent Monitor (for real-time agent activity monitoring)
  agent-monitor:
    build:
      context: .
      dockerfile: docker/agent-monitor.Dockerfile
    container_name: mcp-agent-monitor
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=agent-monitor
      - SERVICE_PORT=8016
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_POOL_URL=http://agent-pool:8011
      - COLLABORATION_URL=http://collaboration-orchestrator:8012
    # ports:
    #   - "8016:8016"
    depends_on:
      - redis
      - consul
      - a2a-communication
      - agent-pool
      - collaboration-orchestrator
    volumes:
      - ./logs/agent-monitor:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8016/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'

  # =====================================
  # FRONTEND APPLICATION
  # =====================================

  # Customer Frontend React/Next.js Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: mcp-frontend
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://localhost/api
      - PORT=3000
    ports:
      - "3002:3000"
    volumes:
      - ./frontend/.next:/app/.next
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Staff Frontend React/Next.js Application
  staff-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: mcp-staff-frontend
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://localhost/api/staff
      - PORT=3001
    ports:
      - "3001:3001"
    volumes:
      - ./frontend/.next:/app/.next
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # =====================================
  # MONITORING & OBSERVABILITY
  # =====================================

  # API Gateway / Load Balancer
  nginx:
    image: nginx:alpine
    container_name: mcp-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - frontend
      - staff-frontend
      - model-router
      - plan-management
      - git-worktree-manager
      - workflow-orchestrator
      - verification-feedback
      - staff-service
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network

  # Metrics Collection
  prometheus:
    image: prom/prometheus:latest
    container_name: mcp-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - mcp-network

  # Metrics Visualization
  grafana:
    image: grafana/grafana:latest
    container_name: mcp-grafana
    restart: unless-stopped
    ports:
      - "3005:3000"
    environment:
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres:5432
      - GF_DATABASE_NAME=grafana_db
      - GF_DATABASE_USER=mcp_user
      - GF_DATABASE_PASSWORD=${POSTGRES_PASSWORD:-mcp_secure_password}
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
      - postgres
    networks:
      - mcp-network

  # Log Aggregation
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    container_name: mcp-elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - mcp-network

  # Log Visualization
  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    container_name: mcp-kibana
    restart: unless-stopped
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - mcp-network

  # Log Shipping
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.8.0
    container_name: mcp-filebeat
    restart: unless-stopped
    user: root
    volumes:
      - ./monitoring/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/var/log/mcp:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - elasticsearch
    networks:
      - mcp-network

  # =====================================
  # DEVELOPMENT & TESTING
  # =====================================

  # Development Tools Container
  dev-tools:
    build:
      context: .
      dockerfile: docker/dev-tools.Dockerfile
    restart: "no"
    profiles:
      - dev
    environment:
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/mcp_database # Default DB for dev-tools
    volumes:
      - .:/workspace
      - ./logs:/workspace/logs
    working_dir: /workspace
    depends_on:
      - redis
      - postgres
    networks:
      - mcp-network
    tty: true
    stdin_open: true

# =====================================
# NETWORKS
# =====================================
networks:
  mcp-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.50.0.0/16

# =====================================
# VOLUMES
# =====================================
volumes:
  redis_data:
    driver: local
  postgres_data:
    driver: local
  consul_data:
    driver: local
  plan_storage:
    driver: local
  git_repositories:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  elasticsearch_data:
    driver: local
