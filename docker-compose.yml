# Docker Compose v2 Configuration for MCP Server Microservices
services:
  mcp-base-builder:
    build:
      context: .
      dockerfile: docker/base.Dockerfile
    image: mcp-base:latest # Tag the built image
    profiles:
      - builder # So it's not always brought up with `up`

  # Fluentd for Logging Requirements
  mcp-fluentd:
    image: fluent/fluentd:v1.16
    container_name: mcp-fluentd
    user: 1000:1000
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    volumes:
      - ./fluentd/conf:/fluentd/etc
      - ./fluentd/log:/var/log/fluentd
    command: ["fluentd", "-c", "/fluentd/etc/fluent.conf", "-v"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "fluentd --dry-run -c /fluentd/etc/fluent.conf || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
    logging:
      driver: "json-file"
    networks:
      mcp-network:
        aliases:
          - fluentd

  # Message Broker for Inter-Service Communication
  redis:
    image: redis:7-alpine
    container_name: mcp-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - mcp-network
    logging:
      driver: "json-file"

  # RabbitMQ for Message Queue
  rabbitmq:
    image: rabbitmq:3.8-management
    container_name: mcp-rabbitmq
    restart: unless-stopped
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    environment:
      - RABBITMQ_DEFAULT_USER=mcp_user
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD:-mcp_secure_password}
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - mcp-network
    logging:
      driver: "json-file"

  # PostgreSQL for Database Service
  postgres:
    image: postgres:15-alpine
    container_name: mcp-postgres
    restart: unless-stopped
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_backups:/backups
      - ./database/init:/docker-entrypoint-initdb.d:ro
      - ./database/scripts:/scripts:ro
    environment:
      # Core PostgreSQL Configuration
      - POSTGRES_DB=mcp_database
      - POSTGRES_USER=mcp_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-mcp_secure_password}
      - POSTGRES_MULTIPLE_DATABASES=plan_management_db,git_worktree_db,workflow_orchestrator_db,verification_feedback_db,staff_db,grafana_db,auth_db,multi_developer_orchestrator_db
      
      # Performance & Corruption Prevention
      - POSTGRES_INITDB_ARGS=--data-checksums
      - POSTGRESQL_SHARED_PRELOAD_LIBRARIES=pg_stat_statements
      - POSTGRESQL_LOG_CHECKPOINTS=on
      - POSTGRESQL_LOG_CONNECTIONS=on
      - POSTGRESQL_LOG_DISCONNECTIONS=on
      - POSTGRESQL_LOG_LOCK_WAITS=on
      - POSTGRESQL_LOG_STATEMENT=all
      - POSTGRESQL_CHECKPOINT_SEGMENTS=32
      - POSTGRESQL_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRESQL_WAL_BUFFERS=16MB
      - POSTGRESQL_EFFECTIVE_CACHE_SIZE=256MB
      - POSTGRESQL_WORK_MEM=4MB
      - POSTGRESQL_MAINTENANCE_WORK_MEM=64MB
      - POSTGRESQL_SYNCHRONOUS_COMMIT=on
      - POSTGRESQL_FSYNC=on
      - POSTGRESQL_FULL_PAGE_WRITES=on
      
      # Backup Configuration
      - POSTGRES_BACKUP_RETENTION_DAYS=7
      - POSTGRES_BACKUP_SCHEDULE=0 2 * * *
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_worker_processes=8
      -c max_parallel_workers=8
      -c max_parallel_maintenance_workers=4
      -c log_statement=all
      -c log_duration=on
      -c log_checkpoints=on
      -c log_connections=on
      -c log_disconnections=on
      -c log_lock_waits=on
      -c deadlock_timeout=1s
      -c shared_preload_libraries=pg_stat_statements
      -c track_activity_query_size=2048
      -c pg_stat_statements.track=all
      -c fsync=on
      -c synchronous_commit=on
      -c full_page_writes=on
      -c wal_log_hints=on
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mcp_user -d mcp_database && psql -U mcp_user -d mcp_database -c 'SELECT 1;' >/dev/null 2>&1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - mcp-network
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # PostgreSQL Monitoring & Backup Service
  postgres-monitor:
    image: postgres:15-alpine
    container_name: mcp-postgres-monitor
    restart: unless-stopped
    environment:
      - POSTGRES_USER=mcp_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-mcp_secure_password}
      - POSTGRES_MONITOR_INTERVAL=300
    volumes:
      - postgres_backups:/backups
      - ./database/scripts:/scripts:ro
    depends_on:
      postgres:
        condition: service_healthy
    command: >
      sh -c "
        while true; do
          echo 'Running PostgreSQL monitoring check...'
          /scripts/monitor.sh || echo 'Monitor check failed'
          echo 'Running backup process...'
          /scripts/backup.sh || echo 'Backup failed'
          echo 'Waiting 1 hour until next check...'
          sleep 3600
        done
      "
    networks:
      - mcp-network
    logging:
      driver: "json-file"

  # Service Discovery and Configuration
  consul:
    image: consul:1.15
    container_name: mcp-consul
    restart: unless-stopped
    ports:
      - "8500:8500"
    command: consul agent -dev -client=0.0.0.0 -ui
    volumes:
      - consul_data:/consul/data
    healthcheck:
      test: ["CMD", "consul", "info"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - mcp-network
    logging:
      driver: "json-file"

  # =====================================
  # CORE MCP MICROSERVICES
  # =====================================

  # Vault Agent for Model Router
  mcp-model-router-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-model-router-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://host.docker.internal:8200
      - VAULT_ROLE_ID_FILE=/run/secrets/model_router_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/model_router_vault_secret_id
    volumes:
      - ./.secrets/model_router_vault_role_id:/run/secrets/model_router_vault_role_id:ro
      - ./.secrets/model_router_vault_secret_id:/run/secrets/model_router_vault_secret_id:ro
      - model_router_secrets:/vault/secrets:rw
      - ./vault/agent-configs/model-router/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/model-router/model_router_config.ctmpl:/vault/secrets/model_router_config.ctmpl:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Model Router Service
  model-router:
    build:
      context: .
      dockerfile: docker/model-router.Dockerfile
    image: mcp-model-router:latest
    restart: unless-stopped
    environment:
      - USE_ABACUS_AI=true
      - PREFER_LOCAL_MODELS=false
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=model-router
      - SERVICE_PORT=8001
      # Observability Configuration
      - LOG_FORMAT=json
      - LOG_LEVEL=INFO
      - TRACING_ENABLED=true
      - TEMPO_OTLP_ENDPOINT=http://tempo:4317
      # Model Router does not use a database directly
    # ports:
    #   - "8001:8001"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
      mcp-model-router-vault-agent:
        condition: service_healthy
    volumes:
      - ./logs/model-router:/app/logs
      - ./config:/app/config:ro
      - model_router_secrets:/vault/secrets:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    command: ["sh", "-c", "source /vault/secrets/.env && python start.py"]
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    logging:
      driver: "json-file"

  # Vault Agent for Plan Management
  mcp-plan-management-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-plan-management-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/plan_management_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/plan_management_vault_secret_id
    volumes:
      - ./.secrets/plan_management_vault_role_id:/run/secrets/plan_management_vault_role_id:ro
      - ./.secrets/plan_management_vault_secret_id:/run/secrets/plan_management_vault_secret_id:ro
      - plan_management_secrets:/vault/secrets:rw
      - ./vault/agent-configs/plan-management/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/plan-management/plan_management_config.ctmpl:/vault/secrets/plan_management_config.ctmpl:ro
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Plan Management Service
  plan-management:
    build:
      context: .
      dockerfile: docker/plan-management.Dockerfile
    image: mcp-plan-management:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=plan-management
      - SERVICE_PORT=8002
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/plan_management_db
      # Observability Configuration
      - LOG_FORMAT=json
      - LOG_LEVEL=INFO
      - TRACING_ENABLED=true
      - TEMPO_OTLP_ENDPOINT=http://tempo:4317
    # ports:
    #   - "8002:8002"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
      mcp-plan-management-vault-agent:
        condition: service_healthy
    volumes:
      - ./logs/plan-management:/app/logs
      - ./config:/app/config:ro
      - ./database/migrations:/app/database/migrations:ro
      - plan_storage:/app/data
      - plan_management_secrets:/vault/secrets:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    command: ["sh", "-c", "source /vault/secrets/.env && python start.py"]
    deploy:
      resources:
        limits:
          memory: 384M
          cpus: '0.4'
        reservations:
          memory: 192M
          cpus: '0.2'
    logging:
      driver: "json-file"

  # Vault Agent for Git Worktree Manager
  mcp-git-worktree-manager-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-git-worktree-manager-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/git_worktree_manager_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/git_worktree_manager_vault_secret_id
    volumes:
      - ./.secrets/git_worktree_manager_vault_role_id:/run/secrets/git_worktree_manager_vault_role_id:ro
      - ./.secrets/git_worktree_manager_vault_secret_id:/run/secrets/git_worktree_manager_vault_secret_id:ro
      - git_worktree_manager_secrets:/vault/secrets:rw
      - ./vault/agent-configs/git-worktree-manager/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/git-worktree-manager/git_worktree_manager_config.ctmpl:/vault/secrets/git_worktree_manager_config.ctmpl:ro
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Git Worktree Manager Service
  git-worktree-manager:
    build:
      context: .
      dockerfile: docker/git-worktree.Dockerfile
    image: mcp-git-worktree-manager:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=git-worktree-manager
      - SERVICE_PORT=8003
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/git_worktree_db
      # Observability Configuration
      - LOG_FORMAT=json
      - LOG_LEVEL=INFO
      - TRACING_ENABLED=true
      - TEMPO_OTLP_ENDPOINT=http://tempo:4317
    # ports:
    #   - "8003:8003"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
      mcp-git-worktree-manager-vault-agent:
        condition: service_healthy
    volumes:
      - ./logs/git-worktree:/app/logs
      - ./config:/app/config:ro
      - git_repositories:/app/repositories
      - /var/run/docker.sock:/var/run/docker.sock:ro  # For Git operations
      - git_worktree_manager_secrets:/vault/secrets:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    command: ["sh", "-c", "source /vault/secrets/.env && python start.py"]
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'
    logging:
      driver: "json-file"

  # Vault Agent for Workflow Orchestrator
  mcp-workflow-orchestrator-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-workflow-orchestrator-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/workflow_orchestrator_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/workflow_orchestrator_vault_secret_id
    volumes:
      - ./.secrets/workflow_orchestrator_vault_role_id:/run/secrets/workflow_orchestrator_vault_role_id:ro
      - ./.secrets/workflow_orchestrator_vault_secret_id:/run/secrets/workflow_orchestrator_vault_secret_id:ro
      - workflow_orchestrator_secrets:/vault/secrets:rw
      - ./vault/agent-configs/workflow-orchestrator/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/workflow-orchestrator/workflow_orchestrator_config.ctmpl:/vault/secrets/workflow_orchestrator_config.ctmpl:ro
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Workflow Orchestrator Service
  workflow-orchestrator:
    build:
      context: .
      dockerfile: docker/workflow-orchestrator.Dockerfile
    image: mcp-workflow-orchestrator:latest
    restart: unless-stopped
    volumes:
      - ./src:/app/src  # Mount source code for development
      - ./logs/workflow-orchestrator:/app/logs
      - ./config:/app/config:ro
      - ./database/migrations:/app/database/migrations:ro
      - workflow_orchestrator_secrets:/vault/secrets:ro
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=workflow-orchestrator
      - SERVICE_PORT=8004
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/workflow_orchestrator_db
      - MODEL_ROUTER_URL=http://model-router:8001
      - PLAN_MANAGEMENT_URL=http://plan-management:8002
      - GIT_WORKTREE_URL=http://git-worktree-manager:8003
      - VERIFICATION_URL=http://verification-feedback:8005
      # Observability Configuration
      - LOG_FORMAT=json
      - LOG_LEVEL=INFO
      - TRACING_ENABLED=true
      - TEMPO_OTLP_ENDPOINT=http://tempo:4317
    # ports:
    #   - "8004:8004"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
      model-router:
        condition: service_healthy
      mcp-workflow-orchestrator-vault-agent:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    command: ["sh", "-c", "source /vault/secrets/.env && python start.py"]
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.6'
        reservations:
          memory: 256M
          cpus: '0.3'
    logging:
      driver: "json-file"

  # Vault Agent for Verification & Feedback Service
  mcp-verification-feedback-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-verification-feedback-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/verification_feedback_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/verification_feedback_vault_secret_id
    volumes:
      - ./.secrets/verification_feedback_vault_role_id:/run/secrets/verification_feedback_vault_role_id:ro
      - ./.secrets/verification_feedback_vault_secret_id:/run/secrets/verification_feedback_vault_secret_id:ro
      - verification_feedback_secrets:/vault/secrets:rw
      - ./vault/agent-configs/verification-feedback/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/verification-feedback/verification_feedback_config.ctmpl:/vault/secrets/verification_feedback_config.ctmpl:ro
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Verification & Feedback Service
  verification-feedback:
    build:
      context: .
      dockerfile: docker/verification-feedback.Dockerfile
    image: mcp-verification-feedback:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=verification-feedback
      - SERVICE_PORT=8005
      - MODEL_ROUTER_URL=http://model-router:8001
      # Observability Configuration
      - LOG_FORMAT=json
      - LOG_LEVEL=INFO
      - TRACING_ENABLED=true
      - TEMPO_OTLP_ENDPOINT=http://tempo:4317
    # ports:
    #   - "8005:8005"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
      model-router:
        condition: service_healthy
      mcp-verification-feedback-vault-agent:
        condition: service_healthy
    volumes:
      - ./logs/verification-feedback:/app/logs
      - ./config:/app/config:ro
      - ./database/migrations:/app/database/migrations:ro
      - verification_feedback_secrets:/vault/secrets:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    command: ["sh", "-c", "source /vault/secrets/.env && python start.py"]
    deploy:
      resources:
        limits:
          memory: 384M
          cpus: '0.4'
        reservations:
          memory: 192M
          cpus: '0.2'
    logging:
      driver: "json-file"

  # Vault Agent for Staff Service
  mcp-staff-service-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-staff-service-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/staff_service_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/staff_service_vault_secret_id
    volumes:
      - ./.secrets/staff_service_vault_role_id:/run/secrets/staff_service_vault_role_id:ro
      - ./.secrets/staff_service_vault_secret_id:/run/secrets/staff_service_vault_secret_id:ro
      - staff_service_secrets:/vault/secrets:rw
      - ./vault/agent-configs/staff-service/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/staff-service/staff_service_config.ctmpl:/vault/secrets/staff_service_config.ctmpl:ro
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Staff Management Service
  staff-service:
    build:
      context: .
      dockerfile: docker/staff-service.Dockerfile
    image: mcp-staff-service:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=staff-service
      - SERVICE_PORT=8006
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/staff_db
    # ports:
    #   - "8006:8006"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
      mcp-staff-service-vault-agent:
        condition: service_healthy
    volumes:
      - ./logs/staff-service:/app/logs
      - ./config:/app/config:ro
      - staff_service_secrets:/vault/secrets:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8006/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    command: ["sh", "-c", "source /vault/secrets/.env && python start.py"]
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'
    logging:
      driver: "json-file"

  # Vault Agent for Auth Service
  mcp-auth-service-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-auth-service-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/auth_service_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/auth_service_vault_secret_id
    volumes:
      - ./.secrets/auth_service_vault_role_id:/run/secrets/auth_service_vault_role_id:ro
      - ./.secrets/auth_service_vault_secret_id:/run/secrets/auth_service_vault_secret_id:ro
      - auth_service_secrets:/vault/secrets:rw
      - ./vault/agent-configs/auth-service/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/auth-service/auth_config.ctmpl:/vault/secrets/auth_config.ctmpl:ro
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Authentication Service - CRITICAL FOR SECURITY
  auth-service:
    build:
      context: .
      dockerfile: docker/auth-service.Dockerfile
    image: mcp-auth-service:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=auth-service
      - SERVICE_PORT=8007
      - DATABASE_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/auth_db
      - JWT_EXPIRY_HOURS=24
      - ENABLE_GITHUB_OAUTH=false
    ports:
      - "8007:8007"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      consul:
        condition: service_healthy
      mcp-auth-service-vault-agent:
        condition: service_healthy
    volumes:
      - ./logs/auth-service:/app/logs
      - ./config:/app/config:ro
      - auth_service_secrets:/vault/secrets:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8007/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    command: ["sh", "-c", "source /vault/secrets/auth_config.env && python start.py"]
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'
    logging:
      driver: "json-file"

  # =====================================
  # AGENT FRAMEWORK SERVICES
  # =====================================

  # A2A Communication Hub
  a2a-communication:
    build:
      context: .
      dockerfile: docker/a2a-communication.Dockerfile
    image: mcp-a2a-communication:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://mcp_user:${RABBITMQ_PASSWORD:-mcp_secure_password}@rabbitmq:5672/%2F
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=a2a-communication
      - SERVICE_PORT=8010
      # A2A Communication does not use a database directly
    # ports:
    #   - "8010:8010"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
    volumes:
      - ./logs/a2a-communication:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'
    logging:
      driver: "json-file"

  # Agent Pool Manager
  agent-pool:
    build:
      context: .
      dockerfile: docker/agent-pool.Dockerfile
    image: mcp-agent-pool:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=agent-pool
      - SERVICE_PORT=8011
      # Agent Pool does not use a database directly
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
    # ports:
    #   - "8011:8011"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
      a2a-communication:
        condition: service_healthy
    volumes:
      - ./logs/agent-pool:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8011/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 384M
          cpus: '0.4'
        reservations:
          memory: 192M
          cpus: '0.2'
    logging:
      driver: "json-file"

  # Collaboration Orchestrator
  collaboration-orchestrator:
    build:
      context: .
      dockerfile: docker/collaboration-orchestrator.Dockerfile
    image: mcp-collaboration-orchestrator:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://mcp_user:${RABBITMQ_PASSWORD:-mcp_secure_password}@rabbitmq:5672/%2F
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=collaboration-orchestrator
      - SERVICE_PORT=8012
      # Multi-developer orchestrator database configuration
      - MULTI_DEVELOPER_ORCHESTRATOR_DB_HOST=postgres
      - MULTI_DEVELOPER_ORCHESTRATOR_DB_USER=mcp_user
      - MULTI_DEVELOPER_ORCHESTRATOR_DB_PASSWORD=${POSTGRES_PASSWORD:-mcp_secure_password}
      - MULTI_DEVELOPER_ORCHESTRATOR_DB_PORT=5432
      - MULTI_DEVELOPER_ORCHESTRATOR_DB_NAME=multi_developer_orchestrator_db
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_POOL_URL=http://agent-pool:8011
    # ports:
    #   - "8012:8012"
    depends_on:
      - redis
      - consul
      - a2a-communication
      - agent-pool
    volumes:
      - ./logs/collaboration-orchestrator:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8012/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    logging:
      driver: "json-file"

  # Specialized Agent Services
  planner-agent:
    build:
      context: .
      dockerfile: docker/planner-agent.Dockerfile
    image: mcp-planner-agent:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://mcp_user:${RABBITMQ_PASSWORD:-mcp_secure_password}@rabbitmq:5672/%2F
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=planner-agent
      - SERVICE_PORT=8013
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_ID=planner-001
      - AGENT_NAME=Primary-Planner
    # ports:
    #   - "8013:8013"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
      a2a-communication:
        condition: service_healthy
    volumes:
      - ./logs/planner-agent:/app/logs
      - ./config:/app/config:ro
    # No health check - this is a background agent using RabbitMQ
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'
    logging:
      driver: "json-file"

  security-agent:
    build:
      context: .
      dockerfile: docker/security-agent.Dockerfile
    image: mcp-security-agent:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://mcp_user:${RABBITMQ_PASSWORD:-mcp_secure_password}@rabbitmq:5672/%2F
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=security-agent
      - SERVICE_PORT=8014
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_ID=security-001
      - AGENT_NAME=Primary-Security
    # ports:
    #   - "8014:8014"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
      a2a-communication:
        condition: service_healthy
    volumes:
      - ./logs/security-agent:/app/logs
      - ./config:/app/config:ro
    # No health check - this is a background agent using RabbitMQ
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'
    logging:
      driver: "json-file"

  developer-agent:
    build:
      context: .
      dockerfile: docker/developer-agent.Dockerfile
    image: mcp-developer-agent:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://mcp_user:${RABBITMQ_PASSWORD:-mcp_secure_password}@rabbitmq:5672/%2F
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=developer-agent
      - SERVICE_PORT=8015
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_ID=developer-001
      - AGENT_NAME=Primary-Developer
      - SPECIALIZATIONS=python,javascript,fastapi
    # ports:
    #   - "8015:8015"
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
      a2a-communication:
        condition: service_healthy
    volumes:
      - ./logs/developer-agent:/app/logs
      - ./config:/app/config:ro
    # No health check - this is a background agent using RabbitMQ
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 384M
          cpus: '0.4'
        reservations:
          memory: 192M
          cpus: '0.2'
    logging:
      driver: "json-file"

  # Agent Monitor (for real-time agent activity monitoring)
  agent-monitor:
    build:
      context: .
      dockerfile: docker/agent-monitor.Dockerfile
    image: mcp-agent-monitor:latest
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - CONSUL_URL=http://consul:8500
      - SERVICE_NAME=agent-monitor
      - SERVICE_PORT=8016
      - A2A_COMMUNICATION_URL=http://a2a-communication:8010
      - AGENT_POOL_URL=http://agent-pool:8011
      - COLLABORATION_URL=http://collaboration-orchestrator:8012
    # ports:
    #   - "8016:8016"
    depends_on:
      - redis
      - consul
      - a2a-communication
      - agent-pool
      - collaboration-orchestrator
    volumes:
      - ./logs/agent-monitor:/app/logs
      - ./config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8016/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'
    logging:
      driver: "json-file"

  # =====================================
  # FRONTEND APPLICATION
  # =====================================

  # Vault Agent for Frontend
  mcp-frontend-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-frontend-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/frontend_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/frontend_vault_secret_id
    volumes:
      - ./.secrets/frontend_vault_role_id:/run/secrets/frontend_vault_role_id:ro
      - ./.secrets/frontend_vault_secret_id:/run/secrets/frontend_vault_secret_id:ro
      - frontend_secrets:/vault/secrets:rw
      - ./vault/agent-configs/frontend/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/frontend/nextauth_config.ctmpl:/vault/secrets/nextauth_config.ctmpl:ro
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Customer Frontend React/Next.js Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    image: mcp-frontend:latest
    restart: unless-stopped
    environment:
      - NODE_ENV=development
      - NEXT_PUBLIC_API_URL=http://nginx/api
      - PORT=3000
    ports:
      - "3002:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /app/.next
      - frontend_secrets:/vault/secrets:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    logging:
      driver: "json-file"

  # Vault Agent for Staff Frontend
  mcp-staff-frontend-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-staff-frontend-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/staff_frontend_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/staff_frontend_vault_secret_id
    volumes:
      - ./.secrets/staff_frontend_vault_role_id:/run/secrets/staff_frontend_vault_role_id:ro
      - ./.secrets/staff_frontend_vault_secret_id:/run/secrets/staff_frontend_vault_secret_id:ro
      - staff_frontend_secrets:/vault/secrets:rw
      - ./vault/agent-configs/staff-frontend/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/staff-frontend/nextauth_config.ctmpl:/vault/secrets/nextauth_config.ctmpl:ro
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Staff Frontend React/Next.js Application
  staff-frontend:
    build:
      context: ./staff-frontend
      dockerfile: Dockerfile.dev
    image: mcp-staff-frontend:latest
    restart: unless-stopped
    environment:
      - NODE_ENV=development
      - NEXT_PUBLIC_API_URL=http://nginx/api/staff
      - PORT=3001
      - NEXTAUTH_SECRET_FILE=/vault/secrets/nextauth_config.env
    ports:
      - "3004:3001"
    volumes:
      - ./staff-frontend:/app
      - /app/node_modules
      - /app/.next
      - staff_frontend_secrets:/vault/secrets:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - mcp-network
    depends_on:
      mcp-staff-frontend-vault-agent:
        condition: service_healthy
    command: ["sh", "-c", "source /vault/secrets/nextauth_config.env && npm run dev"]
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    logging:
      driver: "json-file"

  # =====================================
  # MONITORING & OBSERVABILITY
  # =====================================

  # API Gateway / Load Balancer
  nginx:
    image: nginx:alpine
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - frontend
      - model-router
      - plan-management
      - git-worktree-manager
      - workflow-orchestrator
      - verification-feedback
      - staff-service
      - auth-service
      - staff-frontend
      - mcp-fluentd
    logging:
      driver: "json-file"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - mcp-network

  # Metrics Collection
  prometheus:
    image: prom/prometheus:latest
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - mcp-network
    depends_on:
      - mcp-fluentd
    logging:
      driver: "json-file"

  # Vault Agent for Grafana
  mcp-grafana-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-grafana-vault-agent
    restart: on-failure
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/grafana_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/grafana_vault_secret_id
    volumes:
      - ./.secrets/grafana_vault_role_id:/run/secrets/grafana_vault_role_id:ro
      - ./.secrets/grafana_vault_secret_id:/run/secrets/grafana_vault_secret_id:ro
      - grafana_secrets:/vault/secrets:rw
      - ./vault/agent-configs/grafana/agent-config.hcl:/vault/agent-config.hcl:ro
      - ./vault/agent-configs/grafana/grafana_config.ctmpl:/vault/secrets/grafana_config.ctmpl:ro
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Metrics Visualization
  grafana:
    image: grafana/grafana:latest
    restart: unless-stopped
    ports:
      - "3005:3000"
    environment:
      # Database Configuration for Persistence
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres:5432
      - GF_DATABASE_NAME=grafana_db
      - GF_DATABASE_USER=mcp_user
      - GF_DATABASE_PASSWORD=${POSTGRES_PASSWORD:-mcp_secure_password}
      - GF_DATABASE_SSL_MODE=disable
      - GF_DATABASE_MAX_IDLE_CONN=2
      - GF_DATABASE_MAX_OPEN_CONN=5
      - GF_DATABASE_CONN_MAX_LIFETIME=14400
      
      # Security Configuration
      - GF_SECURITY_ADMIN_USER_FILE=/vault/secrets/grafana_config.env
      - GF_SECURITY_ADMIN_PASSWORD_FILE=/vault/secrets/grafana_config.env
      - GF_SECURITY_SECRET_KEY_FILE=/vault/secrets/grafana_config.env
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_USERS_AUTO_ASSIGN_ORG=true
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Viewer
      
      # Persistence Configuration
      - GF_PATHS_DATA=/var/lib/grafana
      - GF_PATHS_LOGS=/var/lib/grafana/logs
      - GF_PATHS_PLUGINS=/var/lib/grafana/plugins
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      
      # Performance & Reliability
      - GF_LOG_LEVEL=info
      - GF_LOG_MODE=console,file
      - GF_DATABASE_LOG_QUERIES=false
      - GF_FEATURE_TOGGLES_ENABLE=publicDashboards
      
      # Domain Configuration
      - GF_SERVER_DOMAIN=grafana.ethical-ai-insider.com
      - GF_SERVER_ROOT_URL=https://grafana.ethical-ai-insider.com
      - GF_SERVER_SERVE_FROM_SUB_PATH=false
    volumes:
      - grafana_data:/var/lib/grafana
      - grafana_logs:/var/lib/grafana/logs
      - ./monitoring/grafana/dashboards-clean:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - ./monitoring/grafana/init:/docker-entrypoint-initdb.d:ro
      - grafana_secrets:/vault/secrets:ro
    depends_on:
      postgres:
        condition: service_healthy
      prometheus:
        condition: service_started
      
      mcp-grafana-vault-agent:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    logging:
      driver: "json-file"

  # Log Aggregation - Loki (Grafana Stack)
  loki:
    image: grafana/loki:2.9.0
    restart: unless-stopped
    depends_on:
      - consul
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki-config.yml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    logging:
      driver: "json-file"

  # Distributed Tracing - Tempo (Grafana Stack)
  tempo:
    image: grafana/tempo:2.2.0
    restart: unless-stopped
    ports:
      - "3200:3200"   # Tempo HTTP
      - "14268:14268" # Jaeger HTTP
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    volumes:
      - ./monitoring/tempo-config.yml:/etc/tempo.yaml:ro
      - tempo_data:/tmp/tempo
    command: ["-config.file=/etc/tempo.yaml"]
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3200/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mcp-network
    depends_on:
      mcp-fluentd:
        condition: service_healthy
    logging:
      driver: "json-file"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Log Shipping to Loki - Promtail
  promtail:
    image: grafana/promtail:2.9.0
    restart: unless-stopped
    volumes:
      - ./monitoring/promtail-config.yml:/etc/promtail/config.yml:ro
      - ./logs:/var/log/mcp:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      loki:
        condition: service_healthy
    networks:
      - mcp-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 128M
          cpus: '0.15'
    logging:
      driver: "json-file"

  # Legacy Log Aggregation (Elastic Stack) - Optional for migration
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    restart: unless-stopped
    profiles:
      - legacy-logging
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - mcp-network
    depends_on:
      mcp-fluentd:
        condition: service_healthy
    logging:
      driver: "json-file"

  # Legacy Log Visualization
  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    restart: unless-stopped
    profiles:
      - legacy-logging
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
      - mcp-fluentd
    logging:
      driver: "json-file"
    networks:
      - mcp-network

  # Legacy Log Shipping
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.12.0
    restart: unless-stopped
    profiles:
      - legacy-logging
    user: root
    volumes:
      - ./monitoring/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/var/log/mcp:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - elasticsearch
      - mcp-fluentd
    logging:
      driver: "json-file"
    networks:
      - mcp-network

  # Vault Agent for Cloudflare Tunnel
  mcp-cloudflare-tunnel-vault-agent:
    build:
      context: ./docker/vault-agent
      dockerfile: Dockerfile
    container_name: mcp-cloudflare-tunnel-vault-agent
    restart: on-failure
    networks:
      - mcp-network
    depends_on:
      consul:
        condition: service_healthy
    environment:
      - VAULT_ADDR=http://172.50.0.1:8200 # Docker host gateway
      - VAULT_ROLE_ID_FILE=/run/secrets/cloudflare_tunnel_vault_role_id
      - VAULT_SECRET_ID_FILE=/run/secrets/cloudflare_tunnel_vault_secret_id
    volumes:
      - ./.secrets/cloudflare_tunnel_vault_role_id:/run/secrets/cloudflare_tunnel_vault_role_id:ro
      - ./.secrets/cloudflare_tunnel_vault_secret_id:/run/secrets/cloudflare_tunnel_vault_secret_id:ro
      - cloudflare_tunnel_secrets:/vault/secrets:rw
      - ./vault/agent-configs/cloudflare-tunnel/agent-config.hcl:/vault/agent-configs/cloudflare-tunnel/agent-config.hcl:ro
      - ./vault/agent-configs/cloudflare-tunnel/cloudflare_tunnel_config.ctmpl:/vault/secrets/cloudflare_tunnel_config.ctmpl:ro
    healthcheck:
      test: ["CMD-SHELL", "pgrep vault || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"

  # Cloudflare Tunnel Service
  cloudflare-tunnel:
    image: cloudflare/cloudflared:latest
    container_name: mcp-cloudflare-tunnel
    restart: unless-stopped
    command: tunnel --config /etc/cloudflared/config.yml run
    volumes:
      - ./cloudflare-tunnel/config.yml:/etc/cloudflared/config.yml:ro
      - ./.secrets/cloudflare_tunnel_credentials.json:/etc/cloudflared/credentials.json:ro
    environment:
      - TUNNEL_METRICS=0.0.0.0:9080
    networks:
      - mcp-network
    depends_on:
      - nginx
      - mcp-cloudflare-tunnel-vault-agent
      - mcp-fluentd
    logging:
      driver: "json-file"
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
        reservations:
          memory: 64M
          cpus: '0.1'


  # =====================================
  # DEVELOPMENT & TESTING
  # =====================================

  # Development Tools Container
  dev-tools:
    build:
      context: .
      dockerfile: docker/dev-tools.Dockerfile
    image: mcp-dev-tools:latest
    restart: "no"
    profiles:
      - dev
    environment:
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://mcp_user:${POSTGRES_PASSWORD:-mcp_secure_password}@postgres:5432/mcp_database # Default DB for dev-tools
      - PYTHONPATH=/workspace:/home/mcp/.local/lib/python3.11/site-packages
      - PATH=/home/mcp/.local/bin:$PATH
      - RABBITMQ_URL=amqp://mcp_user:${RABBITMQ_PASSWORD:-mcp_secure_password}@rabbitmq:5672/%2F
    volumes:
      - .:/workspace
      - ./logs:/workspace/logs
    working_dir: /workspace
    depends_on:
      - redis
      - postgres
      - rabbitmq
    networks:
      - mcp-network
    tty: true
    stdin_open: true
    command: bash -c "./dev-tools-command.sh"
    logging:
      driver: "json-file"

# =====================================
# NETWORKS
# =====================================
networks:
  mcp-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.50.0.0/16

# =====================================
# VOLUMES
# =====================================


volumes:
  redis_data:
    driver: local
  rabbitmq_data:
    driver: local
  postgres_data:
    driver: local
  postgres_backups:
    driver: local
  consul_data:
    driver: local
  plan_storage:
    driver: local
  git_repositories:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  grafana_logs:
    driver: local
  loki_data:
    driver: local
  tempo_data:
    driver: local
  elasticsearch_data:
    driver: local
  frontend_secrets:
    driver: local
  staff_frontend_secrets:
    driver: local
  auth_service_secrets:
    driver: local
  grafana_secrets:
    driver: local
  model_router_secrets:
    driver: local
  plan_management_secrets:
    driver: local
  git_worktree_manager_secrets:
    driver: local
  workflow_orchestrator_secrets:
    driver: local
  verification_feedback_secrets:
    driver: local
  staff_service_secrets:
    driver: local
  cloudflare_tunnel_secrets:
    driver: local