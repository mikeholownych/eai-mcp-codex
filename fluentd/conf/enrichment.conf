# Log Enrichment and Transformation Rules for MCP Services
# Advanced enrichment for log correlation, normalization, and metadata addition

# Add service metadata and environment information
<filter mcp.**>
  @type record_transformer
  @id add_service_metadata
  enable_ruby true
  <record>
    # Service identification
    service_name ${record["service_name"] || ENV['SERVICE_NAME'] || 'unknown-service'}
    service_version ${record["service_version"] || ENV['SERVICE_VERSION'] || '1.0.0'}
    service_instance ${record["instance_id"] || Socket.gethostname}
    environment ${record["environment"] || ENV['ENVIRONMENT'] || 'development'}
    
    # Infrastructure metadata
    cluster ${record["cluster"] || ENV['CLUSTER_NAME'] || 'unknown'}
    datacenter ${record["datacenter"] || ENV['DATACENTER'] || 'default'}
    region ${record["region"] || ENV['REGION'] || 'us-east-1'}
    availability_zone ${record["availability_zone"] || ENV['AVAILABILITY_ZONE'] || 'unknown'}
    
    # Processing metadata
    processing_node ${record["processing_node"] || ENV['PROCESSING_NODE'] || 'unknown'}
    log_source ${record["log_source"] || ENV['LOG_SOURCE'] || 'application'}
    processed_at ${time}
    processed_by "fluentd"
    processing_version "1.0.0"
    
    # Log classification
    log_type ${classify_log_type(record)}
    log_category ${classify_log_category(record)}
    business_unit ${ENV['BUSINESS_UNIT'] || 'mcp'}
    product ${ENV['PRODUCT'] || 'ai-platform'}
  </record>
</filter>

# Add trace correlation information
<filter mcp.**>
  @type record_transformer
  @id add_trace_correlation
  enable_ruby true
  <record>
    # Generate trace ID if not present
    trace_id ${record["trace_id"] || record["trace_id"] = SecureRandom.uuid}
    
    # Generate span ID if not present
    span_id ${record["span_id"] || record["span_id"] = SecureRandom.uuid[0..16]}
    
    # Add trace sampling info
    trace_sampled ${record["trace_sampled"] || should_sample_trace(record)}
    trace_flags ${record["trace_flags"] || 1}
    
    # Add parent span ID if available
    parent_span_id ${record["parent_span_id"] || nil}
    
    # Add trace context
    trace_context ${generate_trace_context(record)}
    
    # Add correlation ID for business processes
    correlation_id ${record["correlation_id"] || record["request_id"] || SecureRandom.uuid}
    
    # Add session context
    session_id ${record["session_id"] || nil}
    user_id ${record["user_id"] || nil}
    
    # Add operation context
    operation ${record["operation"] || classify_operation(record)}
    operation_type ${record["operation_type"] || classify_operation_type(record)}
    
    # Add workflow context
    workflow_id ${record["workflow_id"] || nil}
    workflow_type ${record["workflow_type"] || nil}
    workflow_step ${record["workflow_step"] || nil}
    
    # Add business process context
    business_process ${record["business_process"] || nil}
    business_function ${record["business_function"] || nil}
    business_transaction ${record["business_transaction"] || nil}
  </record>
</filter>

# Add performance metrics and monitoring data
<filter mcp.**>
  @type record_transformer
  @id add_performance_metrics
  enable_ruby true
  <record>
    # Extract timing metrics
    duration_ms ${record["duration_ms"] || record["response_time"] * 1000 rescue nil}
    duration_seconds ${record["duration_seconds"] || record["duration_ms"] / 1000.0 rescue nil}
    
    # Extract resource usage metrics
    memory_usage_mb ${record["memory_rss_mb"] || record["memory_usage"] rescue nil}
    memory_usage_percent ${record["memory_percent"] rescue nil}
    cpu_usage_percent ${record["cpu_percent"] rescue nil}
    disk_usage_percent ${record["disk_percent"] rescue nil}
    network_io_bytes ${record["network_io"] rescue nil}
    
    # Extract throughput metrics
    throughput_rps ${calculate_throughput_rps(record)}
    throughput_tps ${calculate_throughput_tps(record)}
    throughput_qps ${calculate_throughput_qps(record)}
    
    # Add performance thresholds
    slow_operation_threshold ${record["slow_operation_threshold"] || 10000}  # 10 seconds
    high_memory_threshold ${record["high_memory_threshold"] || 90}  # 90%
    high_cpu_threshold ${record["high_cpu_threshold"] || 80}  # 80%
    
    # Calculate performance flags
    is_slow_operation ${record["duration_ms"] && record["duration_ms"] > (record["slow_operation_threshold"] || 10000)}
    is_high_memory ${record["memory_percent"] && record["memory_percent"] > (record["high_memory_threshold"] || 90)}
    is_high_cpu ${record["cpu_percent"] && record["cpu_percent"] > (record["high_cpu_threshold"] || 80)}
    
    # Add performance score
    performance_score ${calculate_performance_score(record)}
    
    # Add SLA metrics
    sla_compliant ${is_sla_compliant(record)}
    sla_breach_reason ${get_sla_breach_reason(record)}
    sla_target_ms ${get_sla_target_ms(record)}
  </record>
</filter>

# Add security and compliance metadata
<filter mcp.**>
  @type record_transformer
  @id add_security_metadata
  enable_ruby true
  <record>
    # Security classification
    security_level ${classify_security_level(record)}
    security_category ${classify_security_category(record)}
    security_severity ${calculate_security_severity(record)}
    
    # Compliance information
    compliance_frameworks ${get_compliance_frameworks(record)}
    data_classification ${get_data_classification(record)}
    retention_policy ${get_retention_policy(record)}
    
    # Privacy indicators
    contains_pii ${contains_pii_data(record)}
    contains_phi ${contains_phi_data(record)}
    contains_pci ${contains_pci_data(record)}
    
    # Security flags
    is_security_event ${is_security_event(record)}
    is_compliance_event ${is_compliance_event(record)}
    is_audit_event ${is_audit_event(record)}
    requires_investigation ${requires_investigation(record)}
    
    # Access control
    access_level ${get_access_level(record)}
    access_controlled ${is_access_controlled(record)}
    encryption_required ${is_encryption_required(record)}
    
    # Threat detection
    threat_indicators ${extract_threat_indicators(record)}
    anomaly_score ${calculate_anomaly_score(record)}
    risk_score ${calculate_risk_score(record)}
  </record>
</filter>

# Add business context and metrics
<filter mcp.**>
  @type record_transformer
  @id add_business_context
  enable_ruby true
  <record>
    # Business metrics
    business_impact ${assess_business_impact(record)}
    business_priority ${assess_business_priority(record)}
    business_cost ${estimate_business_cost(record)}
    
    # Customer impact
    customer_impact ${assess_customer_impact(record)}
    customer_affected ${is_customer_affected(record)}
    customer_segment ${get_customer_segment(record)}
    
    # Operational metrics
    operational_impact ${assess_operational_impact(record)}
    operational_priority ${assess_operational_priority(record)}
    
    # Financial impact
    financial_impact ${assess_financial_impact(record)}
    financial_cost ${estimate_financial_cost(record)}
    
    # Business process metrics
    process_efficiency ${calculate_process_efficiency(record)}
    process_compliance ${is_process_compliant(record)}
    process_quality ${assess_process_quality(record)}
    
    # Service level metrics
    service_level ${determine_service_level(record)}
    service_priority ${determine_service_priority(record)}
    service_availability ${calculate_service_availability(record)}
  </record>
</filter>

# Add quality and validation metadata
<filter mcp.**>
  @type record_transformer
  @id add_quality_metadata
  enable_ruby true
  <record>
    # Log quality metrics
    log_quality_score ${calculate_log_quality_score(record)}
    log_completeness ${assess_log_completeness(record)}
    log_accuracy ${assess_log_accuracy(record)}
    log_consistency ${assess_log_consistency(record)}
    log_timeliness ${assess_log_timeliness(record)}
    
    # Validation results
    validation_status ${validate_log_record(record)}
    validation_errors ${get_validation_errors(record)}
    validation_warnings ${get_validation_warnings(record)}
    
    # Data quality indicators
    data_quality_score ${calculate_data_quality_score(record)}
    data_integrity ${assess_data_integrity(record)}
    data_freshness ${assess_data_freshness(record)}
    
    # Processing quality
    processing_quality ${assess_processing_quality(record)}
    processing_latency ${calculate_processing_latency(record)}
    processing_success ${is_processing_successful(record)}
    
    # Quality flags
    is_high_quality ${is_high_quality_log(record)}
    requires_attention ${requires_quality_attention(record)}
    needs_enrichment ${needs_further_enrichment(record)}
  </record>
</filter>

# Add alerting and notification metadata
<filter mcp.**>
  @type record_transformer
  @id add_alerting_metadata
  enable_ruby true
  <record>
    # Alert classification
    alert_priority ${determine_alert_priority(record)}
    alert_severity ${determine_alert_severity(record)}
    alert_category ${classify_alert_category(record)}
    
    # Alert conditions
    should_alert ${should_generate_alert(record)}
    alert_reason ${get_alert_reason(record)}
    alert_threshold ${get_alert_threshold(record)}
    
    # Notification settings
    notify_immediately ${should_notify_immediately(record)}
    notification_channels ${get_notification_channels(record)}
    escalation_level ${get_escalation_level(record)}
    
    # Alert lifecycle
    alert_status ${get_alert_status(record)}
    alert_acknowledged ${is_alert_acknowledged(record)}
    alert_resolved ${is_alert_resolved(record)}
    
    # Alert metrics
    alert_frequency ${calculate_alert_frequency(record)}
    alert_trend ${analyze_alert_trend(record)}
    alert_impact ${assess_alert_impact(record)}
  </record>
</filter>

# Add deduplication metadata
<filter mcp.**>
  @type record_transformer
  @id add_deduplication_metadata
  enable_ruby true
  <record>
    # Deduplication keys
    dedup_key ${generate_dedup_key(record)}
    fingerprint ${generate_log_fingerprint(record)}
    hash_signature ${generate_hash_signature(record)}
    
    # Deduplication flags
    is_duplicate ${is_duplicate_log(record)}
    duplicate_count ${get_duplicate_count(record)}
    first_occurrence ${get_first_occurrence_time(record)}
    last_occurrence ${get_last_occurrence_time(record)}
    
    # Pattern detection
    pattern_id ${detect_log_pattern(record)}
    pattern_category ${classify_log_pattern(record)}
    pattern_frequency ${calculate_pattern_frequency(record)}
    
    # Anomaly detection
    is_anomaly ${is_anomalous_log(record)}
    anomaly_score ${calculate_anomaly_score(record)}
    anomaly_reason ${get_anomaly_reason(record)}
    
    # Trend analysis
    trend_direction ${analyze_trend_direction(record)}
    trend_significance ${assess_trend_significance(record)}
    trend_change ${detect_trend_change(record)}
  </record>
</filter>

# Add compression and storage optimization metadata
<filter mcp.**>
  @type record_transformer
  @id add_storage_metadata
  enable_ruby true
  <record>
    # Storage optimization
    compression_algorithm ${determine_compression_algorithm(record)}
    compression_ratio ${estimate_compression_ratio(record)}
    storage_size_bytes ${estimate_storage_size(record)}
    
    # Retention settings
    retention_period ${get_retention_period(record)}
    archival_date ${calculate_archival_date(record)}
    deletion_date ${calculate_deletion_date(record)}
    
    # Storage tier
    storage_tier ${determine_storage_tier(record)}
    storage_class ${determine_storage_class(record)}
    storage_location ${determine_storage_location(record)}
    
    # Backup settings
    backup_required ${is_backup_required(record)}
    backup_frequency ${get_backup_frequency(record)}
    backup_retention ${get_backup_retention(record)}
    
    # Cost optimization
    storage_cost ${estimate_storage_cost(record)}
    processing_cost ${estimate_processing_cost(record)}
    total_cost ${estimate_total_cost(record)}
    
    # Optimization flags
    can_compress ${can_compress_log(record)}
    can_archive ${can_archive_log(record)}
    can_delete ${can_delete_log(record)}
  </record>
</filter>

# Ruby helper functions for log enrichment
<worker 0>
  <script>
    <![CDATA[
      require 'securerandom'
      require 'json'
      require 'digest'
      
      # Log type classification
      def classify_log_type(record)
        message = record["message"] || ""
        level = record["log_level"] || record["level"] || "INFO"
        
        case level.downcase
        when "error", "critical"
          "error"
        when "warning", "warn"
          "warning"
        when "debug", "trace"
          "debug"
        else
          case message.downcase
          when /error|exception|failed|failure/
            "error"
          when /security|auth|login|access|violation/
            "security"
          when /performance|slow|timeout|latency|duration/
            "performance"
          when /audit|compliance|regulation|policy/
            "audit"
          else
            "application"
          end
        end
      end
      
      # Log category classification
      def classify_log_category(record)
        service_name = record["service_name"] || ""
        message = record["message"] || ""
        
        case service_name.downcase
        when /model|llm|ai/
          "ai_operations"
        when /git|repository|version/
          "git_operations"
        when /workflow|orchestration/
          "workflow_management"
        when /plan|planning/
          "plan_management"
        when /verification|validation/
          "verification"
        when /collaboration|consensus/
          "collaboration"
        else
          case message.downcase
          when /database|sql|query/
            "database"
          when /http|request|response/
            "http"
          when /cache|redis/
            "cache"
          when /queue|message|rabbitmq/
            "messaging"
          else
            "general"
          end
        end
      end
      
      # Trace sampling decision
      def should_sample_trace(record)
        # Sample based on error rate, performance, or random sampling
        log_type = classify_log_type(record)
        is_error = log_type == "error"
        is_slow = record["is_slow_operation"] || false
        
        # Always sample errors and slow operations
        return true if is_error || is_slow
        
        # Sample 10% of normal operations
        return rand < 0.1
      end
      
      # Generate trace context
      def generate_trace_context(record)
        {
          "trace_id" => record["trace_id"],
          "span_id" => record["span_id"],
          "trace_flags" => record["trace_flags"] || 1,
          "trace_sampled" => record["trace_sampled"] || false
        }.to_json
      end
      
      # Operation classification
      def classify_operation(record)
        message = record["message"] || ""
        
        case message.downcase
        when /start|begin|init/
          "start"
        when /end|finish|complete/
          "end"
        when /process|handle|execute/
          "process"
        when /create|add|insert/
          "create"
        when /update|modify|change/
          "update"
        when /delete|remove/
          "delete"
        when /read|get|fetch/
          "read"
        when /error|exception|fail/
          "error"
        else
          "unknown"
        end
      end
      
      # Operation type classification
      def classify_operation_type(record)
        service_name = record["service_name"] || ""
        
        case service_name.downcase
        when /model|llm|ai/
          "ai_operation"
        when /git|repository/
          "git_operation"
        when /workflow/
          "workflow_operation"
        when /plan/
          "plan_operation"
        when /verification/
          "verification_operation"
        when /database|sql/
          "database_operation"
        when /http|api/
          "http_operation"
        else
          "general_operation"
        end
      end
      
      # Throughput calculations
      def calculate_throughput_rps(record)
        # Placeholder for actual throughput calculation
        # This would typically be based on request counts and time windows
        0.0
      end
      
      def calculate_throughput_tps(record)
        # Placeholder for actual throughput calculation
        0.0
      end
      
      def calculate_throughput_qps(record)
        # Placeholder for actual throughput calculation
        0.0
      end
      
      # Performance score calculation
      def calculate_performance_score(record)
        score = 100
        
        # Deduct for slow operations
        score -= 20 if record["is_slow_operation"]
        
        # Deduct for high resource usage
        score -= 15 if record["is_high_memory"]
        score -= 15 if record["is_high_cpu"]
        
        # Deduct for errors
        score -= 30 if classify_log_type(record) == "error"
        
        [score, 0].max
      end
      
      # SLA compliance check
      def is_sla_compliant(record)
        duration_ms = record["duration_ms"] || 0
        sla_target = get_sla_target_ms(record)
        
        duration_ms <= sla_target
      end
      
      def get_sla_target_ms(record)
        # Default SLA targets based on operation type
        operation_type = classify_operation_type(record)
        
        case operation_type
        when "http_operation"
          2000  # 2 seconds
        when "database_operation"
          1000  # 1 second
        when "ai_operation"
          30000  # 30 seconds
        else
          5000  # 5 seconds
        end
      end
      
      def get_sla_breach_reason(record)
        return "no_breach" if is_sla_compliant(record)
        
        duration_ms = record["duration_ms"] || 0
        sla_target = get_sla_target_ms(record)
        
        "exceeded_sla_target: #{duration_ms}ms > #{sla_target}ms"
      end
      
      # Security classification
      def classify_security_level(record)
        message = record["message"] || ""
        level = record["log_level"] || "INFO"
        
        case level.downcase
        when "critical"
          "critical"
        when "error"
          "high"
        when "warning"
          "medium"
        else
          case message.downcase
          when /security|auth|violation|breach/
            "high"
          when /unauthorized|forbidden/
            "medium"
          else
            "low"
          end
        end
      end
      
      def classify_security_category(record)
        message = record["message"] || ""
        
        case message.downcase
        when /authentication|login|logout/
          "authentication"
        when /authorization|access|permission/
          "authorization"
        when /attack|malicious|intrusion/
          "threat_detection"
        when /violation|breach|compromise/
          "security_violation"
        else
          "general_security"
        end
      end
      
      def calculate_security_severity(record)
        level = classify_security_level(record)
        message = (record["message"] || "").downcase
        
        severity = 1  # Low
        
        severity += 1 if message.include?("failed")
        severity += 1 if message.include?("unauthorized")
        severity += 1 if message.include?("attack")
        severity += 1 if message.include?("breach")
        
        case level
        when "high"
          severity += 2
        when "critical"
          severity += 3
        end
        
        [severity, 5].min  # Cap at 5
      end
      
      # Compliance framework detection
      def get_compliance_frameworks(record)
        message = record["message"] || ""
        frameworks = []
        
        frameworks << "GDPR" if message.downcase.include?("gdpr")
        frameworks << "HIPAA" if message.downcase.include?("hipaa")
        frameworks << "SOX" if message.downcase.include?("sox")
        frameworks << "PCI" if message.downcase.include?("pci")
        frameworks << "CCPA" if message.downcase.include?("ccpa")
        
        frameworks.empty? ? ["general"] : frameworks
      end
      
      # Data classification
      def get_data_classification(record)
        message = record["message"] || ""
        
        case message.downcase
        when /password|secret|key|token/
          "confidential"
        when /email|phone|address/
          "personal"
        when /ssn|credit|card/
          "restricted"
        else
          "internal"
        end
      end
      
      # Retention policy
      def get_retention_policy(record)
        classification = get_data_classification(record)
        
        case classification
        when "confidential", "restricted"
          "3650d"  # 10 years
        when "personal"
          "1825d"  # 5 years
        else
          "365d"   # 1 year
        end
      end
      
      # PII/PHI/PCI detection
      def contains_pii_data(record)
        message = record["message"] || ""
        
        # Simple PII detection patterns
        pii_patterns = [
          /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/,  # Email
          /\b\d{3}[-.]?\d{2}[-.]?\d{4}\b/,  # SSN
          /\b\d{3}-\d{2}-\d{4}\b/  # Phone
        ]
        
        pii_patterns.any? { |pattern| message.match(pattern) }
      end
      
      def contains_phi_data(record)
        message = record["message"] || ""
        
        # Simple PHI detection patterns
        phi_patterns = [
          /patient|medical|health|diagnosis|treatment/i,
          /doctor|physician|hospital|clinic/i,
          /medication|prescription|pharmacy/i
        ]
        
        phi_patterns.any? { |pattern| message.match(pattern) }
      end
      
      def contains_pci_data(record)
        message = record["message"] || ""
        
        # Simple PCI detection patterns
        pci_patterns = [
          /\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b/,  # Credit card
          /cvv|cvc|exp.*date|card.*number/i
        ]
        
        pci_patterns.any? { |pattern| message.match(pattern) }
      end
      
      # Security event detection
      def is_security_event(record)
        message = record["message"] || ""
        
        security_keywords = [
          "authentication", "authorization", "login", "logout", "access",
          "permission", "security", "violation", "breach", "attack",
          "malicious", "suspicious", "unauthorized", "forbidden"
        ]
        
        security_keywords.any? { |keyword| message.downcase.include?(keyword) }
      end
      
      def is_compliance_event(record)
        message = record["message"] || ""
        
        compliance_keywords = [
          "compliance", "audit", "regulation", "policy", "governance",
          "gdpr", "hipaa", "sox", "pci", "ccpa"
        ]
        
        compliance_keywords.any? { |keyword| message.downcase.include?(keyword) }
      end
      
      def is_audit_event(record)
        message = record["message"] || ""
        
        audit_keywords = [
          "audit", "created", "modified", "deleted", "accessed", "approved",
          "rejected", "reviewed", "verified", "validated"
        ]
        
        audit_keywords.any? { |keyword| message.downcase.include?(keyword) }
      end
      
      def requires_investigation(record)
        severity = calculate_security_severity(record)
        severity >= 4
      end
      
      # Access control
      def get_access_level(record)
        security_level = classify_security_level(record)
        
        case security_level
        when "critical"
          "restricted"
        when "high"
          "confidential"
        when "medium"
          "internal"
        else
          "public"
        end
      end
      
      def is_access_controlled(record)
        classification = get_data_classification(record)
        classification != "public"
      end
      
      def is_encryption_required(record)
        classification = get_data_classification(record)
        ["confidential", "restricted"].include?(classification)
      end
      
      # Threat detection
      def extract_threat_indicators(record)
        message = record["message"] || ""
        indicators = []
        
        # Extract IP addresses
        ip_pattern = /\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b/
        indicators.concat(message.scan(ip_pattern).map { |ip| {"type" => "ip", "value" => ip} })
        
        # Extract URLs
        url_pattern = /https?:\/\/[^\s]+/
        indicators.concat(message.scan(url_pattern).map { |url| {"type" => "url", "value" => url} })
        
        # Extract file paths
        file_pattern = /\/[^\s]+/
        indicators.concat(message.scan(file_pattern).map { |file| {"type" => "file", "value" => file} })
        
        indicators
      end
      
      # Anomaly detection
      def calculate_anomaly_score(record)
        score = 0
        
        # Base score on log type
        case classify_log_type(record)
        when "error"
          score += 30
        when "security"
          score += 50
        when "audit"
          score += 20
        end
        
        # Add for performance issues
        score += 20 if record["is_slow_operation"]
        score += 15 if record["is_high_memory"]
        score += 15 if record["is_high_cpu"]
        
        # Add for security issues
        score += 40 if is_security_event(record)
        score += 30 if requires_investigation(record)
        
        [score, 100].min
      end
      
      def calculate_risk_score(record)
        anomaly_score = calculate_anomaly_score(record)
        security_severity = calculate_security_severity(record)
        
        # Combine anomaly score and security severity
        (anomaly_score * 0.7 + security_severity * 20 * 0.3).to_i
      end
      
      # Business impact assessment
      def assess_business_impact(record)
        log_type = classify_log_type(record)
        service_name = record["service_name"] || ""
        
        case log_type
        when "error"
          "high"
        when "security"
          "critical"
        when "performance"
          "medium"
        else
          case service_name.downcase
          when /model|llm|ai/
            "high"
          when /workflow|orchestration/
            "high"
          when /plan|planning/
            "medium"
          else
            "low"
          end
        end
      end
      
      def assess_business_priority(record)
        impact = assess_business_impact(record)
        
        case impact
        when "critical"
          "p0"
        when "high"
          "p1"
        when "medium"
          "p2"
        else
          "p3"
        end
      end
      
      def estimate_business_cost(record)
        # Placeholder for business cost estimation
        # This would typically be based on downtime, resource usage, etc.
        0.0
      end
      
      # Customer impact assessment
      def assess_customer_impact(record)
        log_type = classify_log_type(record)
        
        case log_type
        when "error", "security"
          "high"
        when "performance"
          "medium"
        else
          "low"
        end
      end
      
      def is_customer_affected(record)
        message = record["message"] || ""
        
        customer_impact_keywords = [
          "customer", "user", "client", "external", "public"
        ]
        
        customer_impact_keywords.any? { |keyword| message.downcase.include?(keyword) }
      end
      
      def get_customer_segment(record)
        # Placeholder for customer segment detection
        # This would typically be based on user context or service type
        "general"
      end
      
      # Operational impact assessment
      def assess_operational_impact(record)
        log_type = classify_log_type(record)
        
        case log_type
        when "error", "security"
          "high"
        when "performance"
          "medium"
        else
          "low"
        end
      end
      
      def assess_operational_priority(record)
        impact = assess_operational_impact(record)
        
        case impact
        when "high"
          "high"
        when "medium"
          "medium"
        else
          "low"
        end
      end
      
      # Financial impact assessment
      def assess_financial_impact(record)
        business_impact = assess_business_impact(record)
        
        case business_impact
        when "critical"
          "critical"
        when "high"
          "high"
        when "medium"
          "medium"
        else
          "low"
        end
      end
      
      def estimate_financial_cost(record)
        # Placeholder for financial cost estimation
        # This would typically be based on downtime, resource costs, etc.
        0.0
      end
      
      # Process metrics
      def calculate_process_efficiency(record)
        # Placeholder for process efficiency calculation
        # This would typically be based on process metrics and KPIs
        1.0
      end
      
      def is_process_compliant(record)
        # Placeholder for process compliance check
        # This would typically be based on compliance rules and regulations
        true
      end
      
      def assess_process_quality(record)
        # Placeholder for process quality assessment
        # This would typically be based on quality metrics and standards
        "good"
      end
      
      # Service level metrics
      def determine_service_level(record)
        business_priority = assess_business_priority(record)
        
        case business_priority
        when "p0"
          "critical"
        when "p1"
          "high"
        when "p2"
          "medium"
        else
          "standard"
        end
      end
      
      def determine_service_priority(record)
        determine_service_level(record)
      end
      
      def calculate_service_availability(record)
        # Placeholder for service availability calculation
        # This would typically be based on uptime and reliability metrics
        99.9
      end
      
      # Log quality assessment
      def calculate_log_quality_score(record)
        score = 100
        
        # Deduct for missing required fields
        required_fields = ["timestamp", "level", "message", "service_name"]
        required_fields.each do |field|
          score -= 10 unless record[field]
        end
        
        # Deduct for validation issues
        score -= 20 if record["validation_status"] == "invalid"
        
        # Deduct for data quality issues
        score -= 15 if record["data_quality_score"] && record["data_quality_score"] < 80
        
        [score, 0].max
      end
      
      def assess_log_completeness(record)
        # Check for completeness of required fields
        required_fields = ["timestamp", "level", "message", "service_name"]
        present_fields = required_fields.select { |field| record[field] }
        
        (present_fields.length.to_f / required_fields.length) * 100
      end
      
      def assess_log_accuracy(record)
        # Placeholder for log accuracy assessment
        # This would typically involve validation against expected formats and values
        100
      end
      
      def assess_log_consistency(record)
        # Placeholder for log consistency assessment
        # This would typically involve checking for consistent field formats and values
        100
      end
      
      def assess_log_timeliness(record)
        # Check if log was processed in a timely manner
        log_timestamp = record["@timestamp"]
        processed_at = record["processed_at"]
        
        return 100 unless log_timestamp && processed_at
        
        # Calculate processing delay
        delay = (Time.parse(processed_at) - Time.parse(log_timestamp)).to_f
        
        # Score based on delay (100% for < 1s, 0% for > 60s)
        delay_threshold = 60.0
        [(delay_threshold - delay) / delay_threshold * 100, 0].max
      end
      
      # Validation functions
      def validate_log_record(record)
        required_fields = ["timestamp", "level", "message"]
        
        missing_fields = required_fields.select { |field| !record[field] || record[field].nil? }
        
        missing_fields.empty? ? "valid" : "invalid"
      end
      
      def get_validation_errors(record)
        required_fields = ["timestamp", "level", "message"]
        
        required_fields.select { |field| !record[field] || record[field].nil? }
      end
      
      def get_validation_warnings(record)
        warnings = []
        
        # Check for potentially missing optional fields
        optional_fields = ["service_name", "trace_id", "operation"]
        optional_fields.each do |field|
          warnings << "missing_optional_field:#{field}" unless record[field]
        end
        
        warnings
      end
      
      # Data quality assessment
      def calculate_data_quality_score(record)
        completeness = assess_log_completeness(record)
        accuracy = assess_log_accuracy(record)
        consistency = assess_log_consistency(record)
        timeliness = assess_log_timeliness(record)
        
        (completeness * 0.3 + accuracy * 0.3 + consistency * 0.2 + timeliness * 0.2).to_i
      end
      
      def assess_data_integrity(record)
        # Placeholder for data integrity assessment
        # This would typically involve checksums and data validation
        "good"
      end
      
      def assess_data_freshness(record)
        # Check if data is recent and up-to-date
        timestamp = record["@timestamp"]
        return "unknown" unless timestamp
        
        age = Time.now - Time.parse(timestamp)
        
        case age
        when 0..300  # 5 minutes
          "fresh"
        when 300..3600  # 1 hour
          "recent"
        when 3600..86400  # 1 day
          "stale"
        else
          "expired"
        end
      end
      
      # Processing quality
      def assess_processing_quality(record)
        # Assess the quality of log processing
        validation_status = record["validation_status"]
        processing_latency = record["processing_latency"] || 0
        
        if validation_status == "invalid"
          "poor"
        elsif processing_latency > 60
          "slow"
        else
          "good"
        end
      end
      
      def calculate_processing_latency(record)
        log_timestamp = record["@timestamp"]
        processed_at = record["processed_at"]
        
        return 0 unless log_timestamp && processed_at
        
        (Time.parse(processed_at) - Time.parse(log_timestamp)).to_f
      end
      
      def is_processing_successful(record)
        validation_status = record["validation_status"]
        validation_status == "valid"
      end
      
      # Quality flags
      def is_high_quality_log(record)
        quality_score = calculate_log_quality_score(record)
        quality_score >= 80
      end
      
      def requires_quality_attention(record)
        quality_score = calculate_log_quality_score(record)
        quality_score < 60
      end
      
      def needs_further_enrichment(record)
        # Check if log needs additional enrichment
        missing_fields = ["trace_id", "operation", "business_process"]
        missing_fields.any? { |field| !record[field] }
      end
      
      # Alerting functions
      def determine_alert_priority(record)
        business_priority = assess_business_priority(record)
        security_severity = calculate_security_severity(record)
        
        case business_priority
        when "p0"
          "critical"
        when "p1"
          security_severity >= 4 ? "critical" : "high"
        when "p2"
          security_severity >= 3 ? "high" : "medium"
        else
          security_severity >= 2 ? "medium" : "low"
        end
      end
      
      def determine_alert_severity(record)
        priority = determine_alert_priority(record)
        
        case priority
        when "critical"
          "critical"
        when "high"
          "major"
        when "medium"
          "minor"
        else
          "warning"
        end
      end
      
      def classify_alert_category(record)
        log_type = classify_log_type(record)
        
        case log_type
        when "error"
          "error"
        when "security"
          "security"
        when "performance"
          "performance"
        when "audit"
          "compliance"
        else
          "operational"
        end
      end
      
      def should_generate_alert(record)
        # Determine if an alert should be generated
        is_error = classify_log_type(record) == "error"
        is_security_event = is_security_event(record)
        is_slow_operation = record["is_slow_operation"] || false
        is_high_resource = record["is_high_memory"] || record["is_high_cpu"] || false
        
        is_error || is_security_event || is_slow_operation || is_high_resource
      end
      
      def get_alert_reason(record)
        reasons = []
        
        reasons << "error" if classify_log_type(record) == "error"
        reasons << "security_event" if is_security_event(record)
        reasons << "slow_operation" if record["is_slow_operation"]
        reasons << "high_memory" if record["is_high_memory"]
        reasons << "high_cpu" if record["is_high_cpu"]
        
        reasons.join(",")
      end
      
      def get_alert_threshold(record)
        # Get appropriate alert threshold based on log type
        case classify_log_type(record)
        when "performance"
          record["slow_operation_threshold"] || 10000
        when "error"
          1  # Any error should trigger
        else
          0  # Default threshold
        end
      end
      
      # Notification settings
      def should_notify_immediately(record)
        priority = determine_alert_priority(record)
        priority == "critical"
      end
      
      def get_notification_channels(record)
        priority = determine_alert_priority(record)
        
        case priority
        when "critical"
          ["pagerduty", "slack", "email"]
        when "high"
          ["slack", "email"]
        when "medium"
          ["email"]
        else
          ["log"]
        end
      end
      
      def get_escalation_level(record)
        priority = determine_alert_priority(record)
        
        case priority
        when "critical"
          3
        when "high"
          2
        when "medium"
          1
        else
          0
        end
      end
      
      # Alert lifecycle
      def get_alert_status(record)
        # Placeholder for alert status determination
        # This would typically involve checking against alert management system
        "active"
      end
      
      def is_alert_acknowledged(record)
        # Placeholder for alert acknowledgment check
        false
      end
      
      def is_alert_resolved(record)
        # Placeholder for alert resolution check
        false
      end
      
      # Alert metrics
      def calculate_alert_frequency(record)
        # Placeholder for alert frequency calculation
        # This would typically involve historical analysis
        0.0
      end
      
      def analyze_alert_trend(record)
        # Placeholder for alert trend analysis
        # This would typically involve time-series analysis
        "stable"
      end
      
      def assess_alert_impact(record)
        determine_alert_priority(record)
      end
      
      # Deduplication functions
      def generate_dedup_key(record)
        # Generate a unique key for deduplication
        message = record["message"] || ""
        service_name = record["service_name"] || ""
        log_level = record["log_level"] || "INFO"
        
        # Create a hash of the key fields
        key_string = "#{service_name}:#{log_level}:#{message}"
        Digest::MD5.hexdigest(key_string)
      end
      
      def generate_log_fingerprint(record)
        # Generate a fingerprint for pattern detection
        message = record["message"] || ""
        
        # Normalize the message by removing variable values
        normalized = message.gsub(/\d+/, "NUM")
                         .gsub(/[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}/, "UUID")
                         .gsub(/\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/, "EMAIL")
                         .gsub(/\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b/, "IP")
        
        Digest::MD5.hexdigest(normalized)
      end
      
      def generate_hash_signature(record)
        # Generate a comprehensive hash signature
        record_string = record.to_json
        Digest::SHA256.hexdigest(record_string)
      end
      
      def is_duplicate_log(record)
        # Placeholder for duplicate detection
        # This would typically involve checking against recent logs
        false
      end
      
      def get_duplicate_count(record)
        # Placeholder for duplicate count retrieval
        # This would typically involve querying a deduplication cache
        0
      end
      
      def get_first_occurrence_time(record)
        # Placeholder for first occurrence time retrieval
        # This would typically involve querying a deduplication cache
        record["@timestamp"]
      end
      
      def get_last_occurrence_time(record)
        # Placeholder for last occurrence time retrieval
        # This would typically involve querying a deduplication cache
        record["@timestamp"]
      end
      
      # Pattern detection
      def detect_log_pattern(record)
        fingerprint = generate_log_fingerprint(record)
        # This would typically involve checking against a pattern database
        fingerprint
      end
      
      def classify_log_pattern(record)
        message = record["message"] || ""
        
        case message.downcase
        when /error|exception|failed/
          "error_pattern"
        when /timeout|slow/
          "performance_pattern"
        when /security|auth/
          "security_pattern"
        else
          "general_pattern"
        end
      end
      
      def calculate_pattern_frequency(record)
        # Placeholder for pattern frequency calculation
        # This would typically involve historical analysis
        0.0
      end
      
      # Anomaly detection
      def is_anomalous_log(record)
        anomaly_score = calculate_anomaly_score(record)
        anomaly_score > 70
      end
      
      def get_anomaly_reason(record)
        reasons = []
        
        reasons << "high_error_rate" if classify_log_type(record) == "error"
        reasons << "security_event" if is_security_event(record)
        reasons << "performance_issue" if record["is_slow_operation"]
        reasons << "resource_issue" if record["is_high_memory"] || record["is_high_cpu"]
        
        reasons.join(",")
      end
      
      # Trend analysis
      def analyze_trend_direction(record)
        # Placeholder for trend direction analysis
        # This would typically involve time-series analysis
        "stable"
      end
      
      def assess_trend_significance(record)
        # Placeholder for trend significance assessment
        # This would typically involve statistical analysis
        "normal"
      end
      
      def detect_trend_change(record)
        # Placeholder for trend change detection
        # This would typically involve change point detection
        false
      end
      
      # Storage optimization
      def determine_compression_algorithm(record)
        # Determine best compression algorithm based on log content
        message = record["message"] || ""
        
        # Use different algorithms based on content type
        if message.length > 1000
          "gzip"  # Better compression for large messages
        else
          "zlib"  # Faster compression for small messages
        end
      end
      
      def estimate_compression_ratio(record)
        # Estimate compression ratio based on log content
        message = record["message"] || ""
        
        # Simple estimation based on message characteristics
        if message.length > 1000
          0.3  # 70% compression for large messages
        else
          0.6  # 40% compression for small messages
        end
      end
      
      def estimate_storage_size(record)
        # Estimate storage size in bytes
        record_string = record.to_json
        compression_ratio = estimate_compression_ratio(record)
        
        (record_string.bytesize * compression_ratio).to_i
      end
      
      # Retention settings
      def get_retention_period(record)
        classification = get_data_classification(record)
        
        case classification
        when "confidential", "restricted"
          "3650d"  # 10 years
        when "personal"
          "1825d"  # 5 years
        else
          "365d"   # 1 year
        end
      end
      
      def calculate_archival_date(record)
        retention_period = get_retention_period(record)
        timestamp = record["@timestamp"]
        
        return nil unless timestamp
        
        # Parse retention period
        period_value = retention_period.to_i
        period_unit = retention_period.gsub(/\d/, "")
        
        # Calculate archival date
        case period_unit
        when "d"
          Time.parse(timestamp) + period_value * 86400
        when "m"
          Time.parse(timestamp) + period_value * 2592000
        when "y"
          Time.parse(timestamp) + period_value * 31536000
        else
          Time.parse(timestamp) + 31536000  # Default to 1 year
        end
      end
      
      def calculate_deletion_date(record)
        archival_date = calculate_archival_date(record)
        return nil unless archival_date
        
        # Delete 30 days after archival
        archival_date + 30 * 86400
      end
      
      # Storage tier
      def determine_storage_tier(record)
        priority = determine_alert_priority(record)
        
        case priority
        when "critical"
          "hot"
        when "high"
          "warm"
        else
          "cold"
        end
      end
      
      def determine_storage_class(record)
        tier = determine_storage_tier(record)
        
        case tier
        when "hot"
          "ssd"
        when "warm"
          "hdd"
        else
          "archive"
        end
      end
      
      def determine_storage_location(record)
        # Determine storage location based on data sensitivity and compliance
        classification = get_data_classification(record)
        
        case classification
        when "confidential", "restricted"
          "secure_region"
        when "personal"
          "compliant_region"
        else
          "standard_region"
        end
      end
      
      # Backup settings
      def is_backup_required(record)
        classification = get_data_classification(record)
        ["confidential", "restricted", "personal"].include?(classification)
      end
      
      def get_backup_frequency(record)
        classification = get_data_classification(record)
        
        case classification
        when "confidential", "restricted"
          "daily"
        when "personal"
          "weekly"
        else
          "monthly"
        end
      end
      
      def get_backup_retention(record)
        classification = get_data_classification(record)
        
        case classification
        when "confidential", "restricted"
          "365d"
        when "personal"
          "90d"
        else
          "30d"
        end
      end
      
      # Cost optimization
      def estimate_storage_cost(record)
        storage_size = estimate_storage_size(record)
        storage_tier = determine_storage_tier(record)
        
        # Cost per GB per month by tier
        cost_per_gb = {
          "hot" => 0.10,
          "warm" => 0.05,
          "cold" => 0.01
        }
        
        (storage_size.to_f / 1024 / 1024 / 1024) * (cost_per_gb[storage_tier] || 0.01)
      end
      
      def estimate_processing_cost(record)
        # Estimate processing cost based on complexity
        complexity_score = record["complexity_score"] || 1
        
        # Base processing cost per log
        base_cost = 0.001  # $0.001 per log
        
        base_cost * complexity_score
      end
      
      def estimate_total_cost(record)
        storage_cost = estimate_storage_cost(record)
        processing_cost = estimate_processing_cost(record)
        
        storage_cost + processing_cost
      end
      
      # Optimization flags
      def can_compress_log(record)
        message = record["message"] || ""
        message.length > 100  # Only compress messages longer than 100 characters
      end
      
      def can_archive_log(record)
        archival_date = calculate_archival_date(record)
        return false unless archival_date
        
        Time.now > archival_date
      end
      
      def can_delete_log(record)
        deletion_date = calculate_deletion_date(record)
        return false unless deletion_date
        
        Time.now > deletion_date
      end
    ]]>
  </script>
</worker>