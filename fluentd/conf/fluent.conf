# Enhanced Fluentd Configuration for MCP Services
# Comprehensive log aggregation, processing, and routing with advanced buffering

# System configuration
<system>
  log_level info
  rpc_endpoint 127.0.0.1:24444
  workers 8
  root_dir /var/log/fluentd
  suppress_config_dump true
  enable_input_metrics true
  enable_size_metrics true
  <log>
    format json
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </log>
</system>

# Input sources for different log types
<source>
  @type forward
  @id forward_input
  port 24224
  bind 0.0.0.0
  tag mcp.*
  <transport>
    tls_version TLSv1_2
    tls_cert_path /etc/fluentd/certs/fluentd.crt
    tls_private_key_path /etc/fluentd/certs/fluentd.key
  </transport>
  <security>
    self_hostname fluentd-server
    shared_key shared_secret_key
  </security>
  <buffer>
    @type file
    path /var/log/fluentd/buffer/forward
    flush_mode interval
    flush_interval 5s
    chunk_limit_size 8MB
    total_limit_size 32GB
    flush_thread_count 4
    overflow_action block
    retry_max_times 3
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 30s
    retry_randomize true
  </buffer>
</source>

# Input for application logs
<source>
  @type tail
  @id app_logs
  path /app/logs/*.log
  pos_file /var/log/fluentd/app.log.pos
  tag mcp.app
  format json
  time_format %Y-%m-%dT%H:%M:%S.%LZ
  read_from_head true
  refresh_interval 5s
  rotate_wait 5s
  enable_stat_watcher true
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
    keep_time_key true
  </parse>
  <buffer>
    @type file
    path /var/log/fluentd/buffer/app
    flush_mode interval
    flush_interval 10s
    chunk_limit_size 8MB
    total_limit_size 16GB
    flush_thread_count 2
    overflow_action block
    retry_max_times 3
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 30s
  </buffer>
</source>

# Input for error logs
<source>
  @type tail
  @id error_logs
  path /app/logs/error.log
  pos_file /var/log/fluentd/error.log.pos
  tag mcp.error
  format json
  time_format %Y-%m-%dT%H:%M:%S.%LZ
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
</source>

# Input for performance logs
<source>
  @type tail
  @id performance_logs
  path /app/logs/performance.log
  pos_file /var/log/fluentd/performance.log.pos
  tag mcp.performance
  format json
  time_format %Y-%m-%dT%H:%M:%S.%LZ
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
</source>

# Input for security logs
<source>
  @type tail
  @id security_logs
  path /app/logs/security.log
  pos_file /var/log/fluentd/security.log.pos
  tag mcp.security
  format json
  time_format %Y-%m-%dT%H:%M:%S.%LZ
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
</source>

# Input for audit logs
<source>
  @type tail
  @id audit_logs
  path /app/logs/audit.log
  pos_file /var/log/fluentd/audit.log.pos
  tag mcp.audit
  format json
  time_format %Y-%m-%dT%H:%M:%S.%LZ
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
</source>

# Input for system logs
<source>
  @type syslog
  @id system_logs
  tag mcp.system
  port 5140
  bind 0.0.0.0
  format json
  <parse>
    @type syslog
    time_format %b %d %H:%M:%S
  </parse>
</source>

# Input for container logs
<source>
  @type tail
  @id container_logs
  path /var/log/containers/*/*.log
  pos_file /var/log/fluentd/container.log.pos
  tag mcp.container
  format json
  read_from_head true
  refresh_interval 5s
  rotate_wait 5s
  enable_stat_watcher true
  <parse>
    @type json
    time_format %Y-%m-%dT%H:%M:%S.%LZ
    keep_time_key true
  </parse>
  <buffer>
    @type file
    path /var/log/fluentd/buffer/container
    flush_mode interval
    flush_interval 10s
    chunk_limit_size 8MB
    total_limit_size 16GB
    flush_thread_count 2
    overflow_action block
    retry_max_times 3
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 30s
  </buffer>
</source>

# Input for Kubernetes logs (if running in K8s)
<source>
  @type kubernetes_metadata
  @id kubernetes_metadata
  tag kubernetes.*
  kubernetes_url https://kubernetes.default.svc:443
  bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
  cache_size 1000
  cache_ttl 60m
  watch false
</source>

# Input for journal logs
<source>
  @type systemd
  @id systemd_logs
  tag mcp.systemd
  path /var/log/journal
  <storage>
    @type local
    persistent true
    path /var/log/fluentd/systemd_pos
  </storage>
  <entry>
    fields_strip_underscores true
    field_map {
      "_SYSTEMD_UNIT": "systemd_unit",
      "_HOSTNAME": "hostname",
      "_PID": "pid"
    }
  </entry>
  <buffer>
    @type file
    path /var/log/fluentd/buffer/systemd
    flush_mode interval
    flush_interval 30s
    chunk_limit_size 8MB
    total_limit_size 8GB
    flush_thread_count 1
    overflow_action block
    retry_max_times 3
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 30s
  </buffer>
</source>

# Input for Windows Event Logs (if applicable)
<source>
  @type windows_eventlog
  @id windows_eventlog
  tag mcp.windows
  channels application,system,security
  read_interval 2
  <storage>
    @type local
    persistent true
    path /var/log/fluentd/windows_eventlog_pos
  </storage>
  <buffer>
    @type file
    path /var/log/fluentd/buffer/windows
    flush_mode interval
    flush_interval 30s
    chunk_limit_size 8MB
    total_limit_size 8GB
    flush_thread_count 1
    overflow_action block
    retry_max_times 3
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 30s
  </buffer>
</source>

# Filter for log enrichment and transformation
<filter mcp.**>
  @type record_transformer
  @id enrich_logs
  enable_ruby true
  <record>
    hostname "#{Socket.gethostname}"
    fluentd_received_at ${time}
    environment "#{ENV['ENVIRONMENT'] || 'development'}"
    cluster "#{ENV['CLUSTER_NAME'] || 'unknown'}"
    datacenter "#{ENV['DATACENTER'] || 'default'}"
    region "#{ENV['REGION'] || 'us-east-1'}"
    availability_zone "#{ENV['AVAILABILITY_ZONE'] || 'unknown'}"
    log_source "#{ENV['LOG_SOURCE'] || 'application'}"
    processing_node "#{ENV['PROCESSING_NODE'] || 'unknown'}"
  </record>
</filter>

# Filter for trace correlation
<filter mcp.**>
  @type record_transformer
  @id trace_correlation
  <record>
    # Generate trace ID if not present
    trace_id ${record["trace_id"] || record["trace_id"] = SecureRandom.uuid}
    
    # Generate span ID if not present
    span_id ${record["span_id"] || record["span_id"] = SecureRandom.uuid[0..16]}
    
    # Add trace sampling info
    trace_sampled ${record["trace_sampled"] || true}
  </record>
</filter>

# Filter for service metadata
<filter mcp.**>
  @type record_transformer
  @id service_metadata
  <record>
    # Add service information
    service_name ${record["service_name"] || "unknown-service"}
    service_version ${record["service_version"] || "1.0.0"}
    service_instance ${record["instance_id"] || Socket.gethostname}
    
    # Add log type classification
    log_type ${record["log_type"] || classify_log_type(record)}
    
    # Add log level normalization
    log_level ${normalize_log_level(record["level"] || record["levelname"] || "INFO")}
  </record>
</filter>

# Filter for sensitive data sanitization
<filter mcp.**>
  @type record_transformer
  @id sanitize_data
  enable_ruby true
  <record>
    message ${sanitize_sensitive_data(record["message"])}
    
    # Sanitize specific fields
    user_id ${sanitize_field(record["user_id"])}
    session_id ${sanitize_field(record["session_id"])}
    ip_address ${sanitize_field(record["ip_address"])}
    
    # Sanitize entire record if it contains sensitive data
    ${sanitize_record(record)}
  </record>
</filter>

# Filter for performance metrics extraction
<filter mcp.performance>
  @type record_transformer
  @id extract_metrics
  <record>
    # Extract performance metrics
    response_time_ms ${record["duration_ms"] || record["response_time"] * 1000 rescue nil}
    memory_usage_mb ${record["memory_rss_mb"] || record["memory_usage"]}
    cpu_usage_percent ${record["cpu_percent"]}
    
    # Calculate throughput metrics
    throughput ${calculate_throughput(record)}
    
    # Add performance thresholds
    slow_operation ${record["duration_ms"] && record["duration_ms"] > 10000}
    high_memory ${record["memory_percent"] && record["memory_percent"] > 90}
    high_cpu ${record["cpu_percent"] && record["cpu_percent"] > 80}
  </record>
</filter>

# Filter for security event classification
<filter mcp.security>
  @type record_transformer
  @id classify_security
  <record>
    # Classify security events
    security_event_type ${classify_security_event(record)}
    security_severity ${calculate_security_severity(record)}
    
    # Add security metadata
    compliance_relevant ${is_compliance_relevant(record)}
    requires_investigation ${requires_investigation(record)}
    
    # Add threat indicators
    threat_indicators ${extract_threat_indicators(record)}
  </record>
</filter>

# Filter for audit trail enhancement
<filter mcp.audit>
  @type record_transformer
  @id enhance_audit
  <record>
    # Add audit metadata
    audit_category ${classify_audit_category(record)}
    audit_action ${record["operation"] || record["action"] || "unknown"}
    audit_result ${record["result"] || record["status"] || "unknown"}
    
    # Add compliance information
    compliance_frameworks ${get_compliance_frameworks(record)}
    data_classification ${get_data_classification(record)}
    
    # Add retention information
    retention_period ${get_retention_period(record)}
  </record>
</filter>

# Filter for log validation
<filter mcp.**>
  @type record_transformer
  @id validate_logs
  enable_ruby true
  <record>
    # Validate required fields
    validation_status ${validate_log_record(record)}
    validation_errors ${get_validation_errors(record)}
    
    # Add quality metrics
    log_quality_score ${calculate_log_quality(record)}
    
    # Add processing metadata
    processed_at ${time}
    processed_by "fluentd"
  </record>
</filter>

# Route logs to different outputs based on type
<match mcp.error>
  @type copy
  @id error_routes
  
  # Send to Elasticsearch with advanced buffering
  <store>
    @type elasticsearch
    @id elasticsearch_error
    host ${ELASTICSEARCH_HOST:-localhost}
    port ${ELASTICSEARCH_PORT:-9200}
    index_name "mcp-error-logs-${Time.at(time).getutc.strftime('%Y.%m.%d')}"
    type_name "_doc"
    include_tag_key true
    tag_key "@tag"
    user ${ELASTICSEARCH_USER:-elastic}
    password ${ELASTICSEARCH_PASSWORD:-changeme}
    scheme ${ELASTICSEARCH_SCHEME:-http}
    ssl_verify ${ELASTICSEARCH_SSL_VERIFY:-false}
    reload_connections true
    reconnect_on_error true
    reload_on_failure true
    <buffer>
      @type file
      path /var/log/fluentd/buffer/elasticsearch_error
      flush_mode interval
      flush_interval 5s
      chunk_limit_size 8MB
      total_limit_size 32GB
      flush_thread_count 4
      overflow_action block
      retry_max_times 5
      retry_type exponential_backoff
      retry_wait 1s
      retry_max_interval 60s
      retry_randomize true
      <secondary>
        @type file
        path /var/log/fluentd/failed/elasticsearch_error
      </secondary>
    </buffer>
  </store>
  
  # Send to error-specific file with compression
  <store>
    @type file
    @id error_file
    path /var/log/fluentd/error
    format json
    <format>
      @type single_value
      message json
    </format>
    <buffer>
      @type file
      path /var/log/fluentd/buffer/error_file
      timekey 1h
      timekey_wait 10m
      flush_mode interval
      flush_interval 30s
      chunk_limit_size 8MB
      total_limit_size 16GB
      flush_thread_count 2
      overflow_action block
      retry_max_times 3
      retry_type exponential_backoff
      retry_wait 1s
      retry_max_interval 30s
      compress gzip
    </buffer>
  </store>
  
  # Send to alerting system with retry
  <store>
    @type webhook
    @id error_alert
    url ${ALERT_WEBHOOK_URL}
    http_method post
    headers {"Content-Type": "application/json"}
    json_array true
    <format>
      @type single_value
      message json
    </format>
    <buffer>
      @type file
      path /var/log/fluentd/buffer/error_alert
      flush_mode interval
      flush_interval 10s
      chunk_limit_size 1MB
      total_limit_size 1GB
      flush_thread_count 1
      overflow_action block
      retry_max_times 5
      retry_type exponential_backoff
      retry_wait 1s
      retry_max_interval 60s
      <secondary>
        @type file
        path /var/log/fluentd/failed/error_alert
      </secondary>
    </buffer>
  </store>
</match>

<match mcp.performance>
  @type copy
  @id performance_routes
  
  # Send to Elasticsearch
  <store>
    @type elasticsearch
    @id elasticsearch_performance
    host ${ELASTICSEARCH_HOST:-localhost}
    port ${ELASTICSEARCH_PORT:-9200}
    index_name "mcp-performance-logs-${Time.at(time).getutc.strftime('%Y.%m.%d')}"
    type_name "_doc"
    include_tag_key true
    tag_key "@tag"
    flush_interval 5s
    retry_limit 3
    retry_wait 1s
  </store>
  
  # Send to Prometheus metrics
  <store>
    @type prometheus
    @id prometheus_metrics
    <metric>
      name mcp_response_time_seconds
      type histogram
      desc Response time in seconds
      key response_time_ms
      <labels>
        tag ${tag}
        service_name ${record["service_name"]}
        operation ${record["operation"]}
      </labels>
      buckets 0.1,0.5,1,2.5,5,10,30
    </metric>
    <metric>
      name mcp_memory_usage_bytes
      type gauge
      desc Memory usage in bytes
      key memory_usage_mb
      <labels>
        tag ${tag}
        service_name ${record["service_name"]}
        instance ${record["service_instance"]}
      </labels>
      scale 1048576  # Convert MB to bytes
    </metric>
    <metric>
      name mcp_cpu_usage_percent
      type gauge
      desc CPU usage percentage
      key cpu_usage_percent
      <labels>
        tag ${tag}
        service_name ${record["service_name"]}
        instance ${record["service_instance"]}
      </labels>
    </metric>
  </store>
  
  # Send to performance-specific file
  <store>
    @type file
    @id performance_file
    path /var/log/fluentd/performance
    format json
    <buffer>
      timekey 1h
      timekey_wait 10m
      flush_mode interval
      flush_interval 30s
    </buffer>
  </store>
</match>

<match mcp.security>
  @type copy
  @id security_routes
  
  # Send to Elasticsearch
  <store>
    @type elasticsearch
    @id elasticsearch_security
    host ${ELASTICSEARCH_HOST:-localhost}
    port ${ELASTICSEARCH_PORT:-9200}
    index_name "mcp-security-logs-${Time.at(time).getutc.strftime('%Y.%m.%d')}"
    type_name "_doc"
    include_tag_key true
    tag_key "@tag"
    flush_interval 1s
    retry_limit 3
    retry_wait 1s
  </store>
  
  # Send to SIEM system
  <store>
    @type syslog
    @id security_syslog
    host ${SIEM_HOST:-localhost}
    port ${SIEM_PORT:-514}
    facility local1
    severity alert
    program mcp-security
    <format>
      @type single_value
      message json
    </format>
  </store>
  
  # Send to security-specific file with long retention
  <store>
    @type file
    @id security_file
    path /var/log/fluentd/security
    format json
    <buffer>
      timekey 1d
      timekey_wait 1h
      flush_mode interval
      flush_interval 60s
    </buffer>
  </store>
</match>

<match mcp.audit>
  @type copy
  @id audit_routes
  
  # Send to Elasticsearch
  <store>
    @type elasticsearch
    @id elasticsearch_audit
    host ${ELASTICSEARCH_HOST:-localhost}
    port ${ELASTICSEARCH_PORT:-9200}
    index_name "mcp-audit-logs-${Time.at(time).getutc.strftime('%Y.%m')}"
    type_name "_doc"
    include_tag_key true
    tag_key "@tag"
    flush_interval 5s
    retry_limit 3
    retry_wait 1s
  </store>
  
  # Send to audit-specific file with very long retention
  <store>
    @type file
    @id audit_file
    path /var/log/fluentd/audit
    format json
    <buffer>
      timekey 1d
      timekey_wait 1h
      flush_mode interval
      flush_interval 60s
    </buffer>
  </store>
  
  # Send to compliance system
  <store>
    @type webhook
    @id compliance_webhook
    url ${COMPLIANCE_WEBHOOK_URL}
    http_method post
    headers {"Content-Type": "application/json"}
    json_array true
    <format>
      @type single_value
      message json
    </format>
    <buffer>
      flush_mode interval
      flush_interval 300s  # 5 minutes
    </buffer>
  </store>
</match>

<match mcp.app>
  @type copy
  @id app_routes
  
  # Send to Elasticsearch
  <store>
    @type elasticsearch
    @id elasticsearch_app
    host ${ELASTICSEARCH_HOST:-localhost}
    port ${ELASTICSEARCH_PORT:-9200}
    index_name "mcp-app-logs-${Time.at(time).getutc.strftime('%Y.%m.%d')}"
    type_name "_doc"
    include_tag_key true
    tag_key "@tag"
    flush_interval 10s
    retry_limit 3
    retry_wait 1s
  </store>
  
  # Send to general logs file
  <store>
    @type file
    @id app_file
    path /var/log/fluentd/app
    format json
    <buffer>
      timekey 1h
      timekey_wait 10m
      flush_mode interval
      flush_interval 30s
    </buffer>
  </store>
</match>

<match mcp.**>
  @type copy
  @id default_routes
  
  # Send to Elasticsearch by default with advanced configuration
  <store>
    @type elasticsearch
    @id elasticsearch_default
    host ${ELASTICSEARCH_HOST:-localhost}
    port ${ELASTICSEARCH_PORT:-9200}
    index_name "mcp-logs-${Time.at(time).getutc.strftime('%Y.%m.%d')}"
    type_name "_doc"
    include_tag_key true
    tag_key "@tag"
    user ${ELASTICSEARCH_USER:-elastic}
    password ${ELASTICSEARCH_PASSWORD:-changeme}
    scheme ${ELASTICSEARCH_SCHEME:-http}
    ssl_verify ${ELASTICSEARCH_SSL_VERIFY:-false}
    reload_connections true
    reconnect_on_error true
    reload_on_failure true
    <buffer>
      @type file
      path /var/log/fluentd/buffer/elasticsearch_default
      flush_mode interval
      flush_interval 10s
      chunk_limit_size 8MB
      total_limit_size 64GB
      flush_thread_count 8
      overflow_action block
      retry_max_times 5
      retry_type exponential_backoff
      retry_wait 1s
      retry_max_interval 60s
      retry_randomize true
      <secondary>
        @type file
        path /var/log/fluentd/failed/elasticsearch_default
      </secondary>
    </buffer>
  </store>
  
  # Send to cloud storage (S3/GCS) with compression
  <store>
    @type s3
    @id cloud_storage
    aws_key_id ${AWS_ACCESS_KEY_ID}
    aws_sec_key ${AWS_SECRET_ACCESS_KEY}
    s3_bucket ${S3_BUCKET:-mcp-logs}
    s3_region ${AWS_REGION:-us-east-1}
    path logs/
    format json
    <buffer>
      @type file
      path /var/log/fluentd/buffer/s3
      timekey 1h
      timekey_wait 10m
      flush_mode interval
      flush_interval 60s
      chunk_limit_size 64MB
      total_limit_size 128GB
      flush_thread_count 4
      overflow_action block
      retry_max_times 5
      retry_type exponential_backoff
      retry_wait 1s
      retry_max_interval 300s
      compress gzip
      <secondary>
        @type file
        path /var/log/fluentd/failed/s3
      </secondary>
    </buffer>
  </store>
  
  # Send to secondary Elasticsearch for disaster recovery
  <store>
    @type elasticsearch
    @id elasticsearch_dr
    host ${ELASTICSEARCH_DR_HOST:-localhost}
    port ${ELASTICSEARCH_DR_PORT:-9200}
    index_name "mcp-logs-dr-${Time.at(time).getutc.strftime('%Y.%m.%d')}"
    type_name "_doc"
    include_tag_key true
    tag_key "@tag"
    user ${ELASTICSEARCH_DR_USER:-elastic}
    password ${ELASTICSEARCH_DR_PASSWORD:-changeme}
    scheme ${ELASTICSEARCH_DR_SCHEME:-http}
    ssl_verify ${ELASTICSEARCH_DR_SSL_VERIFY:-false}
    <buffer>
      @type file
      path /var/log/fluentd/buffer/elasticsearch_dr
      flush_mode interval
      flush_interval 60s
      chunk_limit_size 8MB
      total_limit_size 32GB
      flush_thread_count 2
      overflow_action block
      retry_max_times 3
      retry_type exponential_backoff
      retry_wait 5s
      retry_max_interval 300s
      <secondary>
        @type file
        path /var/log/fluentd/failed/elasticsearch_dr
      </secondary>
    </buffer>
  </store>
  
  # Console output for development (disabled in production)
  <store>
    @type stdout
    @id console_output
    format json
    <match>
      environment development
    </match>
  </store>
</match>

# Ruby helper functions for log processing
<worker 0>
  <script>
    <![CDATA[
      require 'securerandom'
      
      # Classify log type based on content
      def classify_log_type(record)
        message = record["message"] || ""
        
        case message.downcase
        when /error|exception|failed|failure/
          "error"
        when /security|auth|login|access|violation/
          "security"
        when /performance|slow|timeout|latency|duration/
          "performance"
        when /audit|compliance|regulation|policy/
          "audit"
        else
          "application"
        end
      end
      
      # Normalize log levels
      def normalize_log_level(level)
        return "INFO" if level.nil?
        
        level = level.to_s.upcase
        
        case level
        when "DEBUG", "TRACE"
          "DEBUG"
        when "INFO", "INFORMATION"
          "INFO"
        when "WARN", "WARNING"
          "WARNING"
        when "ERROR", "ERR"
          "ERROR"
        when "CRITICAL", "FATAL", "CRIT"
          "CRITICAL"
        else
          "INFO"
        end
      end
      
      # Sanitize sensitive data
      def sanitize_sensitive_data(message)
        return "" if message.nil?
        
        text = message.to_s
        
        # Common sensitive data patterns
        patterns = [
          /api[_-]?key[=:]\s*[^\s]+/i,
          /authorization[=:]\s*[^\s]+/i,
          /bearer\s+[^\s]+/i,
          /token[=:]\s*[^\s]+/i,
          /password[=:]\s*[^\s]+/i,
          /secret[=:]\s*[^\s]+/i,
          /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/,
          /\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b/
        ]
        
        patterns.each do |pattern|
          text = text.gsub(pattern, "[REDACTED]")
        end
        
        text
      end
      
      # Sanitize specific fields
      def sanitize_field(value)
        return nil if value.nil?
        "[REDACTED]"
      end
      
      # Sanitize entire record
      def sanitize_record(record)
        return if !record.is_a?(Hash)
        
        sensitive_keys = ["password", "token", "secret", "key", "auth", "credential"]
        
        record.each do |key, value|
          if sensitive_keys.any? { |sk| key.to_s.downcase.include?(sk) }
            record[key] = "[REDACTED]"
          end
        end
      end
      
      # Calculate throughput metrics
      def calculate_throughput(record)
        # Implementation depends on specific metrics
        # This is a placeholder
        0.0
      end
      
      # Classify security events
      def classify_security_event(record)
        message = (record["message"] || "").downcase
        
        case message
        when /authentication|login|logout/
          "authentication"
        when /authorization|access|permission/
          "authorization"
        when /attack|malicious|intrusion/
          "threat_detection"
        when /violation|breach|compromise/
          "security_violation"
        else
          "general_security"
        end
      end
      
      # Calculate security severity
      def calculate_security_severity(record)
        message = (record["message"] || "").downcase
        level = record["log_level"] || "INFO"
        
        severity = 1  # Low
        
        severity += 1 if message.include?("failed")
        severity += 1 if message.include?("unauthorized")
        severity += 1 if message.include?("attack")
        severity += 1 if message.include?("breach")
        severity += 1 if level == "ERROR"
        severity += 2 if level == "CRITICAL"
        
        [severity, 5].min  # Cap at 5
      end
      
      # Check if compliance relevant
      def is_compliance_relevant(record)
        message = (record["message"] || "").downcase
        compliance_keywords = ["gdpr", "hipaa", "sox", "pci", "compliance", "audit", "regulation"]
        
        compliance_keywords.any? { |kw| message.include?(kw) }
      end
      
      # Check if investigation required
      def requires_investigation(record)
        severity = record["security_severity"] || 1
        severity >= 4
      end
      
      # Extract threat indicators
      def extract_threat_indicators(record)
        # Implementation depends on specific threat detection logic
        # This is a placeholder
        []
      end
      
      # Classify audit category
      def classify_audit_category(record)
        message = (record["message"] || "").downcase
        
        case message
        when /create|add|insert/
          "data_creation"
        when /update|modify|change/
          "data_modification"
        when /delete|remove/
          "data_deletion"
        when /access|read|view/
          "data_access"
        when /login|logout|auth/
          "authentication"
        else
          "general_audit"
        end
      end
      
      # Get compliance frameworks
      def get_compliance_frameworks(record)
        # Implementation depends on specific compliance requirements
        # This is a placeholder
        []
      end
      
      # Get data classification
      def get_data_classification(record)
        # Implementation depends on specific data classification rules
        # This is a placeholder
        "internal"
      end
      
      # Get retention period
      def get_retention_period(record)
        # Implementation depends on specific retention policies
        # This is a placeholder
        "365d"
      end
      
      # Validate log record
      def validate_log_record(record)
        required_fields = ["timestamp", "level", "message"]
        
        missing_fields = required_fields.select { |field| !record.key?(field) || record[field].nil? }
        
        missing_fields.empty? ? "valid" : "invalid"
      end
      
      # Get validation errors
      def get_validation_errors(record)
        required_fields = ["timestamp", "level", "message"]
        
        required_fields.select { |field| !record.key?(field) || record[field].nil? }
      end
      
      # Calculate log quality score
      def calculate_log_quality(record)
        score = 100
        
        # Deduct for missing fields
        score -= 10 if !record.key?("service_name")
        score -= 10 if !record.key?("trace_id")
        score -= 5 if !record.key?("operation")
        score -= 5 if !record.key?("request_id")
        
        # Deduct for validation issues
        score -= 20 if record["validation_status"] == "invalid"
        
        [score, 0].max  # Ensure non-negative
      end
    ]]>
  </script>
</worker>
